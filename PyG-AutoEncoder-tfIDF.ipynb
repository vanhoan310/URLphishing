{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db6ea671-60fe-4cfe-ae8f-d7c37cbdebf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import run_ML\n",
    "from sklearn.metrics import f1_score\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9132d59-ef74-44d2-91aa-e89d9b5fc54d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51996577-859a-466e-a40f-b60f6a51a8d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ac7c73-a600-48e9-8931-3afa1679e506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tldextract\n",
    "# !pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b90b7a-8d1a-4472-889c-bc87ef4a9706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_dir = \"data/URLdatasetX2_1.csv\"\n",
    "# df = pd.read_csv(data_dir,index_col=0)\n",
    "# # smalldata = df.sample(n = 20000, random_state=1)\n",
    "# smalldata = df\n",
    "# # get labels of urls\n",
    "# labels = smalldata.iloc[:,-1].values\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "5aca4692-f14f-4210-88cb-68aa5654bbc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_dir = \"data/LegitPhish_50_50.csv\"\n",
    "# data_dir = \"data/LegitPhish_80_20.csv\"\n",
    "data_dir = \"data/LegitPhish_90_10.csv\"\n",
    "df = pd.read_csv(data_dir,index_col=0)\n",
    "# smalldata = df.sample(n = 5000, random_state=1)\n",
    "smalldata = df\n",
    "# get labels of urls\n",
    "labels = smalldata.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "fcd08658-b0b2-44af-8058-95c255eed904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 5644, 1: 11373})"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "counter = collections.Counter(labels)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "4c92b77a-0aba-471e-b046-dad1477491f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33166833166833165"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5644.0/(5644 + 11373)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a837d-63e3-40cf-afc9-9593072d34e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conventional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "3cd8d0e0-af10-42b5-9bd9-bced95415a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "155e525e-f2fc-49d2-8be2-3b64ea9b67b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'domain': 'www.example.com', 'num_subdomains': 2, 'contains_ip': 0, 'path_length': 20, 'num_path_segments': 3, 'uses_https': 0, 'file_extension': 'html', 'count_special_characters': 11, 'count_non_alphanumeric_characters': 11, 'TLD': 'com', 'count_obfuscated_characters': 0, 'letter_ratio_in_url': 0.7380952380952381, 'digit_ratio_in_url': 0.0, 'count_equals_in_url': 2, 'NoOfAmpersandInURL': 0, 'CharContinuationRate': 0.11904761904761904, 'ratio_obfuscated_characters': 0.0, 'NoOfQMarkInURL': 0}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "url = \"http://www.example.com/path/to/==file.html\"\n",
    "url_features = extract_features(url)\n",
    "print(url_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2377259d-5b19-4f0f-8758-96b1b330d117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(url_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "66cae4c3-a178-435c-9830-a4f5fd8fdc06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get numerical and catergorical features\n",
    "phish_url = []\n",
    "for link in list(smalldata.iloc[:,0]):\n",
    "    url_features = extract_features(link)\n",
    "    phish_url.append(list(url_features.values())[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "664a11ec-0a42-4a01-91f2-208aa5796b64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phish_url_df = pd.DataFrame(phish_url, columns = list(url_features.keys())[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "383162e7-d058-471e-b921-d2e983efe066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# phish_url_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "aabba33a-d06a-4a3f-b021-ff8cbde0088c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phish_url_df.iloc[:,5] = pd.Categorical(phish_url_df.iloc[:,5]).codes\n",
    "phish_url_df.iloc[:,8] = pd.Categorical(phish_url_df.iloc[:,8]).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1cc59152-00c2-41ec-acf5-afeae474f39e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_subdomains</th>\n",
       "      <th>contains_ip</th>\n",
       "      <th>path_length</th>\n",
       "      <th>num_path_segments</th>\n",
       "      <th>uses_https</th>\n",
       "      <th>file_extension</th>\n",
       "      <th>count_special_characters</th>\n",
       "      <th>count_non_alphanumeric_characters</th>\n",
       "      <th>TLD</th>\n",
       "      <th>count_obfuscated_characters</th>\n",
       "      <th>letter_ratio_in_url</th>\n",
       "      <th>digit_ratio_in_url</th>\n",
       "      <th>count_equals_in_url</th>\n",
       "      <th>NoOfAmpersandInURL</th>\n",
       "      <th>CharContinuationRate</th>\n",
       "      <th>ratio_obfuscated_characters</th>\n",
       "      <th>NoOfQMarkInURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_subdomains  contains_ip  path_length  num_path_segments  uses_https  \\\n",
       "0               2            0            1                  1           0   \n",
       "1               3            0           80                  7           0   \n",
       "\n",
       "   file_extension  count_special_characters  \\\n",
       "0               0                         6   \n",
       "1               0                        16   \n",
       "\n",
       "   count_non_alphanumeric_characters  TLD  count_obfuscated_characters  \\\n",
       "0                                  6   40                            0   \n",
       "1                                 16   40                            0   \n",
       "\n",
       "   letter_ratio_in_url  digit_ratio_in_url  count_equals_in_url  \\\n",
       "0             0.823529                 0.0                    0   \n",
       "1             0.864407                 0.0                    0   \n",
       "\n",
       "   NoOfAmpersandInURL  CharContinuationRate  ratio_obfuscated_characters  \\\n",
       "0                   0              0.058824                          0.0   \n",
       "1                   0              0.050847                          0.0   \n",
       "\n",
       "   NoOfQMarkInURL  \n",
       "0               0  \n",
       "1               0  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phish_url_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "503ed86a-9489-487f-8054-8c0d4653da36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:  0 , fold:  0\n",
      "Train freq:  [1319, 2681]\n",
      "kNN, LightGBM, Run:  0 , fold:  1\n",
      "Train freq:  [1340, 2660]\n",
      "kNN, LightGBM, Run:  0 , fold:  2\n",
      "Train freq:  [1331, 2669]\n",
      "kNN, LightGBM, Run:  0 , fold:  3\n",
      "Train freq:  [1357, 2643]\n",
      "kNN, LightGBM, Run:  0 , fold:  4\n",
      "Train freq:  [1333, 2667]\n",
      "kNN, LightGBM, ['kNN', 'LightGBM']\n",
      "[0.77 0.86]\n"
     ]
    }
   ],
   "source": [
    "# test on URLs features\n",
    "run_ML(phish_url_df, labels, \"URLdatasetX2\", \"manual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c999c69c-2338-4ea3-a5df-2bbd10c0dd49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:  0 , fold:  0\n",
      "Train freq:  [1319, 2681]\n",
      "kNN, LightGBM, Run:  0 , fold:  1\n",
      "Train freq:  [1340, 2660]\n",
      "kNN, LightGBM, Run:  0 , fold:  2\n",
      "Train freq:  [1331, 2669]\n",
      "kNN, LightGBM, Run:  0 , fold:  3\n",
      "Train freq:  [1357, 2643]\n",
      "kNN, LightGBM, Run:  0 , fold:  4\n",
      "Train freq:  [1333, 2667]\n",
      "kNN, LightGBM, ['kNN', 'LightGBM']\n",
      "[0.74 0.82]\n"
     ]
    }
   ],
   "source": [
    "## test on numerical URLs features\n",
    "from utils import extract_numerical_features\n",
    "phish_url = []\n",
    "for link in list(smalldata.iloc[:,0]):\n",
    "    url_features = extract_numerical_features(link)\n",
    "    phish_url.append(list(url_features.values()))\n",
    "run_ML(np.array(phish_url), labels, \"URLdatasetX2\", \"manual_numerical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2f45c022-60fb-4a87-9b53-10f2a093f7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8150066161646513\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "n_samples = len(smalldata.index)\n",
    "train_idx = list(np.random.choice(list(range(n_samples)), int(0.8*n_samples), replace=False))\n",
    "test_idx = list(set(list(range(n_samples))).difference(set(train_idx)))\n",
    "data_df = np.array(phish_url)\n",
    "import lightgbm as lgb\n",
    "model = lgb.LGBMClassifier(verbose=-1)\n",
    "model.fit(data_df[train_idx], labels[train_idx])\n",
    "y_predict=model.predict(data_df[test_idx]) \n",
    "print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c2e9e-0c05-4e9d-8abe-d89cf1519c87",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PyG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79b7fe-f017-4894-a558-ecf95e00c992",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract graph features from URLs for PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "2be46503-1dab-4c89-9a14-2859346a62ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9153484801573358\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(1, 4))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(1, 4))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(analyzer='char', ngram_range=(1, 4))"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "# List of URLs\n",
    "urls = list(smalldata['url'])\n",
    "# Tokenization and N-grams Generation\n",
    "# You can adjust ngram_range to extract different n-grams (e.g., (1, 1) for unigrams, (2, 2) for bigrams, etc.)\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 4)) #5\n",
    "X_counts = vectorizer.fit_transform(urls)\n",
    "# TF-IDF Transformation\n",
    "transformer = TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(X_counts)\n",
    "# Extracted Features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "X_counts_data = X_counts.toarray() # not necessary\n",
    "# Train lgb\n",
    "model_lgb = lgb.LGBMClassifier(verbose=-1)\n",
    "model_lgb.fit(X_counts_data[train_idx], labels[train_idx])\n",
    "y_predict=model_lgb.predict(X_counts_data[test_idx]) \n",
    "print(f1_score(y_predict, labels[test_idx], average='macro'))\n",
    "feature_imp_gain = pd.DataFrame(sorted(zip(model_lgb.booster_.feature_importance(importance_type='gain'),\n",
    "                                           feature_names), reverse=True), columns=['Value', 'Feature'])\n",
    "feature_imp_split = pd.DataFrame(sorted(zip(model_lgb.booster_.feature_importance(importance_type='split'),\n",
    "                                            feature_names), reverse=True), columns=['Value', 'Feature'])\n",
    "top_ngrams_features = list(set(list(feature_imp_gain.iloc[:200,1]) + list(feature_imp_split.iloc[:200,1])))\n",
    "cv = CountVectorizer(analyzer='char', ngram_range=(1, 4))\n",
    "cv.fit(top_ngrams_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "8ec2dd2e-e01b-4b9d-bb3e-f8a36ec4e787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_feature_CountVectorizer(model, url):\n",
    "    return model.transform([url]).toarray().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "f60ec269-beed-4faf-8010-9465bafd2fc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract_feature_CountVectorizer(cv, urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "2676fb9b-966b-4acf-a071-89bea32f7bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import ConnectionError\n",
    "import traceback\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "406988f9-e960-4a0d-87da-64ae205ff5b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# return root and hyperlinks features\n",
    "def get_graph_features_CountVectorizer(idx):\n",
    "    url = smalldata.iloc[idx,0]\n",
    "    root_feature = extract_feature_CountVectorizer(cv, url) # dict\n",
    "    hyperlink_data = [list(root_feature)]\n",
    "    try:    \n",
    "        # find all hyperlinks\n",
    "        reqs = requests.get(url, allow_redirects=False)\n",
    "        soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "        urls = []\n",
    "        count = 0;\n",
    "        for link in soup.find_all('a'):\n",
    "            # print(link.get('href'))\n",
    "            weblink = link.get('href')\n",
    "            if (weblink is not None) and ('http' in weblink):\n",
    "                urls.append(weblink)\n",
    "            count += 1\n",
    "            if count > 50:\n",
    "                break\n",
    "        # extract numerical features in from hyperlinks\n",
    "        if len(urls) > 0:\n",
    "            for link in urls:\n",
    "                try:\n",
    "                    url_features = extract_feature_CountVectorizer(cv, link)\n",
    "                    datalinkssss = list(url_features)\n",
    "                    hyperlink_data.append(datalinkssss)\n",
    "                except ValueError as ve:\n",
    "                    # datalinkssss = list(np.zeros(15))#raw_graph_features\n",
    "                    error_here = 1;\n",
    "                # hyperlink_data.append(datalinkssss)\n",
    "        else:\n",
    "            # hyperlink_data.append(list(np.zeros(15)))#raw_graph_features\n",
    "            error_here = 1;\n",
    "    \n",
    "    # except ConnectionError as e:\n",
    "    #     # print(\"No rep\", end = ',')\n",
    "    #     # hyperlink_data.append(list(np.zeros(15))) #raw_graph_features\n",
    "    #     error_here = 1; #v2\n",
    "    except Exception as e:\n",
    "        #logging.error(traceback.format_exc())\n",
    "        error_here = 1\n",
    "    \n",
    "    return (idx,  hyperlink_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "d758fb40-1b9a-415b-9072-b4d3b15c3b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results = [get_graph_features_CountVectorizer(i) for i in range(n_test_samples)]\n",
    "# results = []\n",
    "# for i in range(213, n_test_samples):\n",
    "#     print(i, end =',')\n",
    "#     results.append(get_graph_features_CountVectorizer(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "6b604d43-5fd2-4ebb-9b8c-76b27fc66b3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperlink_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "766cd5a5-dcb4-4f03-a0ed-ee9d3ba25d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist! Process the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanhoan310/miniconda3/envs/py38/lib/python3.8/site-packages/bs4/__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/home/vanhoan310/miniconda3/envs/py38/lib/python3.8/site-packages/bs4/__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/home/vanhoan310/miniconda3/envs/py38/lib/python3.8/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/home/vanhoan310/miniconda3/envs/py38/lib/python3.8/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/home/vanhoan310/miniconda3/envs/py38/lib/python3.8/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_name_0123 = data_dir.split('/')[-1][:-4]\n",
    "# data_file = \"data/raw_graph_features_v2.pickle\" # first version \n",
    "data_file = \"data/\"+data_name_0123+\"raw_graph_features_CountVectorizer.pickle\" # first version \n",
    "my_file = Path(data_file)\n",
    "if my_file.is_file():\n",
    "    print(\"File exist! Load the data\")\n",
    "    with open(data_file, \"rb\") as fp:   # Unpickling\n",
    "        results = pickle.load(fp)\n",
    "else:\n",
    "    print(\"File does not exist! Process the data\")\n",
    "    n_test_samples = int(smalldata.shape[0]) # how many link we want to test\n",
    "    from joblib import Parallel, delayed\n",
    "    results = Parallel(n_jobs=30)(delayed(get_graph_features_CountVectorizer)(i) for i in range(n_test_samples)) # test on 100 links\n",
    "    with open(data_file, \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "cc340201-b13f-460d-b649-b81213128397",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/LegitPhish_90_10raw_graph_features_CountVectorizer.pickle'"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9aaf9a-c170-49a2-99af-bcc50ee7d6cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Graph data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "836dd0a7-8ac7-487f-a204-7de4c6c60c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transfer data object to GPU.\n",
    "# device = torch.device('cuda')\n",
    "# data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "05cfcd5d-7cfb-4489-8df7-c845a56503c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "class GraphClassificationDataset(Dataset):\n",
    "    def __init__(self, graphs):\n",
    "        self.graphs = graphs\n",
    "        # self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        graph = self.graphs[idx]\n",
    "        # label = self.labels[idx]\n",
    "        return graph\n",
    "    \n",
    "    def get(): pass\n",
    "\n",
    "    def len(): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be4688-e4d2-4401-917c-31bf7ce340b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create dataset class for PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "e8bd6a84-04a7-4f20-b6be-04f31d2a1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have a list of graphs represented as Data objects and a corresponding list of labels\n",
    "# Only take the url with more than 4 hyperlinks\n",
    "graphs = []\n",
    "labels_list = []\n",
    "for i in range(len(results)):\n",
    "    idx, graph_feature = results[i]\n",
    "    n_hyperlinks = len(graph_feature)-1\n",
    "    child_id = [i+1 for i in range(n_hyperlinks)]\n",
    "    source_id = list(np.zeros(n_hyperlinks).astype(int))\n",
    "    # edge_index = torch.tensor([source_id + child_id,\n",
    "    #                            child_id + source_id], dtype=torch.long)\n",
    "    edge_index = torch.tensor([source_id,\n",
    "                               child_id], dtype=torch.long)\n",
    "    x = torch.tensor(graph_feature, dtype=torch.float)\n",
    "    y = torch.tensor([labels[idx]], dtype=torch.int64)\n",
    "    data = Data(x=x, edge_index=edge_index, y = y)\n",
    "    if n_hyperlinks > -1:\n",
    "        graphs.append(data)\n",
    "        labels_list.append(labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "795e7cc1-6f35-4ed2-863a-34cece9a52da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train freq:  [1670, 3330]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train freq: \", [len(list(group)) for key, group in groupby(sorted(labels_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "44a3a5ee-7d6e-412c-8ec3-7a653bbbb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GraphClassificationDataset(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "6dc5a361-8c19-48a4-a651-f46ca40ffdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: GraphClassificationDataset(5000):\n",
      "====================\n",
      "Number of graphs: 5000\n",
      "Number of features: 415\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e69f8dd0-7d89-4d23-9051-1e86c544d508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[398, 3833, 4836, 4572, 636, 2545, 1161, 2230, 148, 2530]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = dataset.shuffle()\n",
    "n_samples = len(dataset)\n",
    "np.random.seed(0) #TODOXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "train_idx = list(np.random.choice(list(range(n_samples)), int(0.8*n_samples), replace=False))\n",
    "test_idx = list(set(list(range(n_samples))).difference(set(train_idx)))\n",
    "train_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "def48b62-971c-4e4f-ac9c-126a06f2739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = dataset[:int(0.8*n_samples)]\n",
    "# test_dataset = dataset[int(0.8*n_samples):]\n",
    "train_dataset = [dataset[idx] for idx in train_idx]\n",
    "test_dataset = [dataset[idx] for idx in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "5b8c74ac-9ca9-498f-b5dc-ee77ab28f254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1000)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f115489a-ccce-4f40-9355-452d4cc6b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc21d4-1976-46ce-8640-d6994adafd0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build and train PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5f975f01-7eb2-4b3e-9bd7-b8c897785e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "        self.linconcat = Linear(2*hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "bd41cc6b-7c71-4d42-9b24-5419f05f98c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train F1: 0.8936, Test F1: 0.8685\n",
      "Epoch: 010, Train F1: 0.9641, Test F1: 0.8743\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        # print(data.x.shape)\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    # for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "    #     out = model(data.x, data.edge_index, data.batch)  \n",
    "    #     pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    #     correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    # return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        true_labels += data.y.tolist()\n",
    "        pred_labels += pred.tolist()\n",
    "        # print(pred_labels)\n",
    "    return f1_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "for epoch in range(0, 20):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train F1: {train_acc:.4f}, Test F1: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "9bb46af3-23a3-4c2b-a73a-496e34bdd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['kNN', 'LightGBM'] min, max, avg of child features [0.82 0.92]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7128b95-c87e-4614-b2b6-65b372846199",
   "metadata": {},
   "source": [
    "### GNN for dim. reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d0b6c-94b6-46fe-ae04-8a22b8244a20",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Net 1: 0.87, 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "93e113dd-b47e-4696-854f-6cd2bf99f479",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "class GCNdimReduce(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNdimReduce, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def dimReduce(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        # x = x.relu()\n",
    "        # x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # # 3. Apply a final classifier\n",
    "        # x = self.lin1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b201f-f9cd-4c2c-8fb2-299019c33656",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Net 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "49cab21a-1649-4cfb-8b32-775372f8855e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "class GCNdimReduceV2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNdimReduceV2, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "       \n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def dimReduce(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4c5b2-1bbe-4de7-b566-e55cddcae716",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c54c9876-1471-4154-bacf-7e7e0131e641",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train F1: 0.4000, Test F1: 0.3987\n"
     ]
    }
   ],
   "source": [
    "n_hidden_channels = 2\n",
    "model = GCNdimReduce(hidden_channels=n_hidden_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        # print(data.x.shape)\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        true_labels += data.y.tolist()\n",
    "        pred_labels += pred.tolist()\n",
    "    return f1_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train F1: {train_acc:.4f}, Test F1: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2d3201ea-2ee2-40ca-8d89-8dfbf13896d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "model.eval()\n",
    "dim_vec = torch.empty((0, n_hidden_channels), dtype=torch.float32)\n",
    "# data = next(iter(test_loader))\n",
    "for data in all_data_loader:\n",
    "    # model.eval()\n",
    "    dim_x = model.dimReduce(data.x, data.edge_index, data.batch)\n",
    "    # print(dim_x.shape)\n",
    "    dim_vec = torch.cat((dim_vec, dim_x), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "6c089665-699b-4361-abc9-90a5add2357f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phish_url_vectorizer = []\n",
    "for link in list(smalldata.iloc[:,0]):\n",
    "    url_features = extract_feature_CountVectorizer(cv, link)\n",
    "    phish_url_vectorizer.append(list(url_features))\n",
    "concatGNN = np.concatenate((np.array(phish_url_vectorizer), dim_vec.detach().numpy()),axis=1)\n",
    "# run_ML(concatGNN, labels, \"URLdatasetX2\", \"concatGNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "2d73d8c8-fbb4-4aad-86de-173e4a05532e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.919023630273301\n"
     ]
    }
   ],
   "source": [
    "model_lgb = lgb.LGBMClassifier(verbose=-1)\n",
    "model_lgb.fit(concatGNN[train_idx], labels[train_idx])\n",
    "y_predict=model_lgb.predict(concatGNN[test_idx]) \n",
    "print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f4b8358b-0dd5-4200-90a6-3354ba9a4543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7738222055030346\n"
     ]
    }
   ],
   "source": [
    "model_lgb = lgb.LGBMClassifier(verbose=-1)\n",
    "graph_embedding = dim_vec.detach().numpy()\n",
    "model_lgb.fit(graph_embedding[train_idx], labels[train_idx])\n",
    "y_predict=model_lgb.predict(graph_embedding[test_idx]) \n",
    "print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "1af5ad50-e1db-4188-aa52-0d842fc8a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phish_url_vectorizer = []\n",
    "# for link in list(smalldata.iloc[:,0]):\n",
    "#     url_features = extract_feature_CountVectorizer(cv, link)\n",
    "#     phish_url_vectorizer.append(list(url_features))\n",
    "# concatGNN = np.array(phish_url_vectorizer)\n",
    "# # run_ML(concatGNN, labels, \"URLdatasetX2\", \"concatGNN\")\n",
    "# model_lgb = lgb.LGBMClassifier(verbose=-1)\n",
    "# model_lgb.fit(concatGNN[train_idx], labels[train_idx])\n",
    "# y_predict=model_lgb.predict(concatGNN[test_idx]) \n",
    "# print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "8f2c5d10-1693-4e61-8ff3-2414cc7aa668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_ML(np.array(phish_url_vectorizer), labels, \"URLdatasetX2\", \"concatGNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "4b385cb2-9578-47b8-a27b-9121a17c5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_ML(dim_vec.detach().numpy(), labels, \"URLdatasetX2\", \"concatGNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "9072a734-e891-49f3-8e63-57c509ca5751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx, vec = results[0]; vec = np.array(vec); n_features_counter = int(vec.shape[1]);\n",
    "n_features_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "7f056c1d-1429-46fa-886d-3a36d0a95200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for i in range(len(results)):\n",
    "#     idx, vec = results[i]\n",
    "#     vec = np.array(vec)\n",
    "#     if vec.shape[0] <= 1:\n",
    "#         count += 1\n",
    "# print(count, len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "9c1876a8-77f4-4055-8132-db0c6a1a3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlink_features = np.zeros((smalldata.shape[0], 3*n_features_counter))\n",
    "for idx, hyper_np in results:\n",
    "    # print(idx, hyper_np)\n",
    "    hyper_np = np.array(hyper_np)\n",
    "    if hyper_np.shape[0] >= 2:\n",
    "        hyperlink_features[idx, :] = np.hstack((hyper_np.min(axis=0),hyper_np.max(axis=0), hyper_np.mean(axis=0)))\n",
    "    # hyperlink_features[idx, :] = hyper_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "1469d86b-66ca-4b2b-9ceb-b6c7563658aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatGNN_graph = np.concatenate((np.array(phish_url_vectorizer), hyperlink_features),axis=1)\n",
    "concatGNN_graph = np.concatenate((np.array(phish_url_vectorizer), hyperlink_features,  dim_vec.detach().numpy()),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "2f26d260-c6b8-4c0d-bbe3-c633b4f4c618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9122060591325959\n"
     ]
    }
   ],
   "source": [
    "model_lgb2 = lgb.LGBMClassifier(verbose=-1)\n",
    "model_lgb2.fit(concatGNN_graph[train_idx], labels[train_idx])\n",
    "y_predict=model_lgb2.predict(concatGNN_graph[test_idx]) \n",
    "print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "0827b97c-e921-4050-8927-f6fa2cef8ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 2])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a797f3ec-0737-43bf-a68f-02e88fbf1fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
