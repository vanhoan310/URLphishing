{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1dabd84-d826-462d-8672-6c50b95672f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanhoan310/miniconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# TODO: use manual HTML features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import run_ML\n",
    "from sklearn.metrics import f1_score\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2362c08a-bc2b-4b59-952a-9303ecdea868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import gensim\n",
    "import urllib\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from numpy import load, save,  asarray\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import networkx as nx\n",
    "from scipy.sparse import csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f29055-f291-41fb-8bb2-f0dfa71b1759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_dir = \"Le/DLM-main/DLM-main/tmp\"\n",
    "data_dir = \"Le/DLM-main/DLM-main/html_4000_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a412cfc-46d1-4227-8ed5-7d2ecd92d8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc2vec = gensim.models.doc2vec.Doc2Vec.load(\"doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "190a4016-74ab-4e74-946a-7775cde2ac0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import generate_filename, convert, read_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a0e1699-73e3-4864-ac7b-4372b03b94de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def callTag(tag, node):\n",
    "    node_from = node\n",
    "    node = node + 1\n",
    "\n",
    "    for tag in tag.children:\n",
    "        attr_node = 1\n",
    "        if (tag.name is not None):\n",
    "            nodes_list.append(convert([str(node_from), str(node)]))\n",
    "            attr_list.append(str(node) + \",nname,\" + str(tag.name))\n",
    "            attr_list.append(str(node) + \",value,\" + \"\")\n",
    "            for attr in tag.attrs:\n",
    "                nodes_list.append(convert([str(node), str(node) + \"_\" + str(attr_node)]))\n",
    "                attr_list.append(str(node) + \"_\" + str(attr_node) + \",nname,\" + str(attr))\n",
    "                attr_list.append(str(node) + \"_\" + str(attr_node) + \",value,\" + str(tag.get(attr)))\n",
    "                attr_node = attr_node + 1\n",
    "            callTag(tag, node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "716dcd3a-17a1-40dd-9e3a-70af987fb029",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_udst_inputs(url, rec_id, phishing_flag):\n",
    "    node = 0\n",
    "    f = codecs.open(Path(data_dir + '/' + generate_filename(rec_id)), 'r', encoding='utf-8', errors='ignore')\n",
    "    soup = BeautifulSoup(f)\n",
    "    callTag(soup, node)\n",
    "\n",
    "    nodes_list.pop(0)\n",
    "    attr_list.pop(1)\n",
    "    attr_list.insert(1, '1, value, ' + url)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(nodes_list)\n",
    "\n",
    "    A = nx.adjacency_matrix(G) # N*N Adjacency matrix\n",
    "\n",
    "    df_features = pd.DataFrame(0.0, index=G.nodes(), columns=['nname', 'value'])\n",
    "\n",
    "    attr_val_list = []\n",
    "    for x in attr_list:\n",
    "        y = x.split(',')\n",
    "        if y[1] == \"value\":\n",
    "            attr_val_list.append(y[2])\n",
    "\n",
    "    for x in attr_list:\n",
    "        y = x.split(',')\n",
    "        test_corpus = list(read_corpus([y[2]], tokens_only=True))\n",
    "        vector = doc2vec.infer_vector(test_corpus[0])\n",
    "        # print(test_corpus[0])\n",
    "\n",
    "        if y[1] == \"nname\":\n",
    "            df_features.loc[y[0]][\"nname\"] = vector\n",
    "\n",
    "        else:\n",
    "            df_features.loc[y[0]][\"value\"] = vector\n",
    "\n",
    "    X = df_features.values # N*d Feature Matrix\n",
    "    # y = to_categorical(phishing_flag, num_classes=2).tolist() # Label\n",
    "    y = phishing_flag\n",
    "\n",
    "    X_ = np.array([np.array(ai, dtype=np.float32) for ai in X.tolist()])\n",
    "\n",
    "    # url_token = url_tokenizer([url])[0]\n",
    "    url_token = None\n",
    "\n",
    "    return X_, A, y, url_token, url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "495d20bc-ba14-40e3-b16d-4515c6a7cc19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete empty HTML files\n",
    "# data_type = 'Validation' # Train, Test, Validation\n",
    "# df_te = pd.read_csv('Le/DLM-main/DLM-main/data1/'+data_type+'_Data.csv')\n",
    "# exist_idx = []\n",
    "# for idx, data in df_te.iterrows():\n",
    "#     my_file = Path(data_dir + '/' + generate_filename(data['rec_id']))\n",
    "#     if my_file.is_file():\n",
    "#         if os.stat(data_dir + '/' + generate_filename(data['rec_id'])).st_size > 0:\n",
    "#             exist_idx.append(idx)\n",
    "        \n",
    "# newtr = df_te.iloc[exist_idx,]\n",
    "# newtr.to_csv('Le/DLM-main/DLM-main/data1/'+data_type+'_NewData.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bafb136-e6cb-453c-88db-2408c7c4b50b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Le/DLM-main/DLM-main/data1/Train_NewData.csv')\n",
    "df_val = pd.read_csv('Le/DLM-main/DLM-main/data1/Validation_NewData.csv')\n",
    "df_test = pd.read_csv('Le/DLM-main/DLM-main/data1/Test_NewData.csv')\n",
    "# exist_idx = []\n",
    "# for idx, data in df_tr.iterrows():\n",
    "#     my_file = Path(data_dir + '/' + generate_filename(data['rec_id']))\n",
    "#     if my_file.is_file():\n",
    "#         print(data['rec_id'], data['url'], data['result'], 'Y')\n",
    "#     else:\n",
    "#         print('=========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "664a1e84-cf7f-40d5-8e12-5f443ffefee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _, data in df_test.iloc[3:4,:].iterrows():\n",
    "    # node = 0\n",
    "    nodes_list = []\n",
    "    attr_list = []\n",
    "    X, A, y, X_A, _ = get_udst_inputs(data['url'], data['rec_id'], data['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "948d4dbe-a5a9-45f8-8927-c48e34262f12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((151, 2), (151, 151))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9132d59-ef74-44d2-91aa-e89d9b5fc54d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51996577-859a-466e-a40f-b60f6a51a8d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "684aa1a4-fbf6-4259-a5ce-b9a4c03c8399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # data_dir = \"data/URLdatasetX2_1.csv\"\n",
    "# data_dir = \"data/URLdatasetX2_1sub10.csv\"\n",
    "# df = pd.read_csv(data_dir,index_col=0)\n",
    "# # n_subsample = 3000 # all\n",
    "# # smalldata = df.sample(n = n_subsample, random_state=1) #PC\n",
    "# n_subsample = 'full'; smalldata = df;\n",
    "# # get labels of urls\n",
    "# labels = smalldata.iloc[:,-1].values\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# labels = label_encoder.fit_transform(labels)\n",
    "smalldata = pd.concat([df_train, df_test])\n",
    "labels = np.concatenate([df_train['result'], df_test['result']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1191cc9-98c5-44a7-8c0c-29f92121e08d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rec_id</th>\n",
       "      <th>url</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>https://kluban.net</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>https://1ki174.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rec_id                 url  result\n",
       "0       7  https://kluban.net       0\n",
       "1      15  https://1ki174.com       0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smalldata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcd08658-b0b2-44af-8058-95c255eed904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 182, 1: 357}), 539)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "counter = collections.Counter(labels)\n",
    "counter, len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a837d-63e3-40cf-afc9-9593072d34e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conventional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cd8d0e0-af10-42b5-9bd9-bced95415a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66cae4c3-a178-435c-9830-a4f5fd8fdc06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get numerical and catergorical features\n",
    "phish_url = []\n",
    "for link in list(smalldata.iloc[:,1]):\n",
    "    url_features = extract_features(link)\n",
    "    phish_url.append(list(url_features.values())[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "664a11ec-0a42-4a01-91f2-208aa5796b64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phish_url_df = pd.DataFrame(phish_url, columns = list(url_features.keys())[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "383162e7-d058-471e-b921-d2e983efe066",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_subdomains</th>\n",
       "      <th>contains_ip</th>\n",
       "      <th>path_length</th>\n",
       "      <th>num_path_segments</th>\n",
       "      <th>uses_https</th>\n",
       "      <th>file_extension</th>\n",
       "      <th>count_special_characters</th>\n",
       "      <th>count_non_alphanumeric_characters</th>\n",
       "      <th>TLD</th>\n",
       "      <th>count_obfuscated_characters</th>\n",
       "      <th>letter_ratio_in_url</th>\n",
       "      <th>digit_ratio_in_url</th>\n",
       "      <th>count_equals_in_url</th>\n",
       "      <th>NoOfAmpersandInURL</th>\n",
       "      <th>CharContinuationRate</th>\n",
       "      <th>ratio_obfuscated_characters</th>\n",
       "      <th>NoOfQMarkInURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>net</td>\n",
       "      <td>0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>com</td>\n",
       "      <td>0</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_subdomains  contains_ip  path_length  num_path_segments  uses_https  \\\n",
       "0               1            0            0                  1           1   \n",
       "1               1            0            0                  1           1   \n",
       "\n",
       "  file_extension  count_special_characters  count_non_alphanumeric_characters  \\\n",
       "0                                        4                                  4   \n",
       "1                                        4                                  4   \n",
       "\n",
       "   TLD  count_obfuscated_characters  letter_ratio_in_url  digit_ratio_in_url  \\\n",
       "0  net                            0             0.777778            0.000000   \n",
       "1  com                            0             0.555556            0.222222   \n",
       "\n",
       "   count_equals_in_url  NoOfAmpersandInURL  CharContinuationRate  \\\n",
       "0                    0                   0              0.111111   \n",
       "1                    0                   0              0.111111   \n",
       "\n",
       "   ratio_obfuscated_characters  NoOfQMarkInURL  \n",
       "0                          0.0               0  \n",
       "1                          0.0               0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phish_url_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aabba33a-d06a-4a3f-b021-ff8cbde0088c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phish_url_df.iloc[:,5] = pd.Categorical(phish_url_df.iloc[:,5]).codes\n",
    "phish_url_df.iloc[:,8] = pd.Categorical(phish_url_df.iloc[:,8]).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cc59152-00c2-41ec-acf5-afeae474f39e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_subdomains</th>\n",
       "      <th>contains_ip</th>\n",
       "      <th>path_length</th>\n",
       "      <th>num_path_segments</th>\n",
       "      <th>uses_https</th>\n",
       "      <th>file_extension</th>\n",
       "      <th>count_special_characters</th>\n",
       "      <th>count_non_alphanumeric_characters</th>\n",
       "      <th>TLD</th>\n",
       "      <th>count_obfuscated_characters</th>\n",
       "      <th>letter_ratio_in_url</th>\n",
       "      <th>digit_ratio_in_url</th>\n",
       "      <th>count_equals_in_url</th>\n",
       "      <th>NoOfAmpersandInURL</th>\n",
       "      <th>CharContinuationRate</th>\n",
       "      <th>ratio_obfuscated_characters</th>\n",
       "      <th>NoOfQMarkInURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_subdomains  contains_ip  path_length  num_path_segments  uses_https  \\\n",
       "0               1            0            0                  1           1   \n",
       "1               1            0            0                  1           1   \n",
       "\n",
       "   file_extension  count_special_characters  \\\n",
       "0               0                         4   \n",
       "1               0                         4   \n",
       "\n",
       "   count_non_alphanumeric_characters  TLD  count_obfuscated_characters  \\\n",
       "0                                  4   50                            0   \n",
       "1                                  4   14                            0   \n",
       "\n",
       "   letter_ratio_in_url  digit_ratio_in_url  count_equals_in_url  \\\n",
       "0             0.777778            0.000000                    0   \n",
       "1             0.555556            0.222222                    0   \n",
       "\n",
       "   NoOfAmpersandInURL  CharContinuationRate  ratio_obfuscated_characters  \\\n",
       "0                   0              0.111111                          0.0   \n",
       "1                   0              0.111111                          0.0   \n",
       "\n",
       "   NoOfQMarkInURL  \n",
       "0               0  \n",
       "1               0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phish_url_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "503ed86a-9489-487f-8054-8c0d4653da36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # test on URLs features\n",
    "# run_ML(phish_url_df, labels, \"URLdatasetX2\", \"manual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c999c69c-2338-4ea3-a5df-2bbd10c0dd49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:  0 , fold:  0\n",
      "Train freq:  [151, 280]\n",
      "kNN, LightGBM, Run:  0 , fold:  1\n",
      "Train freq:  [141, 290]\n",
      "kNN, LightGBM, Run:  0 , fold:  2\n",
      "Train freq:  [150, 281]\n",
      "kNN, LightGBM, Run:  0 , fold:  3\n",
      "Train freq:  [140, 291]\n",
      "kNN, LightGBM, Run:  0 , fold:  4\n",
      "Train freq:  [146, 286]\n",
      "kNN, LightGBM, ['kNN', 'LightGBM']\n",
      "[0.97 0.98]\n"
     ]
    }
   ],
   "source": [
    "## test on numerical URLs features\n",
    "from utils import extract_numerical_features\n",
    "phish_url = []\n",
    "for link in list(smalldata.iloc[:,1]):\n",
    "    url_features = extract_numerical_features(link)\n",
    "    phish_url.append(list(url_features.values()))\n",
    "run_ML(np.array(phish_url), labels, \"Cuvation\", \"manual_numerical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c115e42-4077-4a5f-8a8d-1b8dc3012801",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9753197622050082\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "n_samples = len(smalldata.index)\n",
    "train_idx = range(len(df_train.index))\n",
    "test_idx = list(set(list(range(n_samples))).difference(set(train_idx)))\n",
    "#data_df = np.array(phish_url_df)\n",
    "data_df = np.array(phish_url)\n",
    "import lightgbm as lgb\n",
    "model = lgb.LGBMClassifier(verbose=-1)\n",
    "model.fit(data_df[train_idx], labels[train_idx])\n",
    "y_predict=model.predict(data_df[test_idx]) \n",
    "print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79b7fe-f017-4894-a558-ecf95e00c992",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract graph features from URLs for PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2be46503-1dab-4c89-9a14-2859346a62ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(1, 4))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(1, 4))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(analyzer='char', ngram_range=(1, 4))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "# List of URLs\n",
    "urls = list(smalldata['url'])\n",
    "# Tokenization and N-grams Generation\n",
    "# You can adjust ngram_range to extract different n-grams (e.g., (1, 1) for unigrams, (2, 2) for bigrams, etc.)\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 4)) #5\n",
    "X_counts = vectorizer.fit_transform(urls)\n",
    "# TF-IDF Transformation\n",
    "transformer = TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(X_counts)\n",
    "# Extracted Features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "X_counts_data = X_counts.toarray() # not necessary\n",
    "# Train lgb\n",
    "model_lgb = lgb.LGBMClassifier(verbose=-1)\n",
    "model_lgb.fit(X_counts_data[train_idx], labels[train_idx])\n",
    "y_predict=model_lgb.predict(X_counts_data[test_idx]) \n",
    "print(f1_score(y_predict, labels[test_idx], average='macro'))\n",
    "feature_imp_gain = pd.DataFrame(sorted(zip(model_lgb.booster_.feature_importance(importance_type='gain'),\n",
    "                                           feature_names), reverse=True), columns=['Value', 'Feature'])\n",
    "feature_imp_split = pd.DataFrame(sorted(zip(model_lgb.booster_.feature_importance(importance_type='split'),\n",
    "                                            feature_names), reverse=True), columns=['Value', 'Feature'])\n",
    "top_ngrams_features = list(set(list(feature_imp_gain.iloc[:200,1]) + list(feature_imp_split.iloc[:200,1])))\n",
    "cv = CountVectorizer(analyzer='char', ngram_range=(1, 4))\n",
    "cv.fit(top_ngrams_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ec2dd2e-e01b-4b9d-bb3e-f8a36ec4e787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_feature_CountVectorizer(model, url):\n",
    "    return model.transform([url]).toarray().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f60ec269-beed-4faf-8010-9465bafd2fc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract_feature_CountVectorizer(cv, urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2aff0302-b0ae-4da5-9f44-171a1010f762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import ConnectionError\n",
    "import traceback\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "406988f9-e960-4a0d-87da-64ae205ff5b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# return root and hyperlinks features\n",
    "def get_graph_features_CountVectorizer(idx):\n",
    "    url = smalldata.iloc[idx,1]\n",
    "    root_feature = extract_feature_CountVectorizer(cv, url) # dict\n",
    "    hyperlink_data = [list(root_feature)]\n",
    "    try:    \n",
    "        # find all hyperlinks\n",
    "        rec_id = smalldata.iloc[idx, 0]\n",
    "        f = codecs.open(Path(data_dir + '/' + generate_filename(rec_id)), 'r', encoding='utf-8', errors='ignore')\n",
    "        soup = BeautifulSoup(f)\n",
    "        urls = []\n",
    "        count = 0;\n",
    "        for link in soup.find_all('a'):\n",
    "            # print(link.get('href'))\n",
    "            weblink = link.get('href')\n",
    "            if (weblink is not None) and ('http' in weblink):\n",
    "                urls.append(weblink)\n",
    "            count += 1\n",
    "            if count > 50:\n",
    "                break\n",
    "        # extract numerical features in from hyperlinks\n",
    "        if len(urls) > 0:\n",
    "            for link in urls:\n",
    "                try:\n",
    "                    url_features = extract_feature_CountVectorizer(cv, link)\n",
    "                    datalinkssss = list(url_features)\n",
    "                    hyperlink_data.append(datalinkssss)\n",
    "                except ValueError as ve:\n",
    "                    # datalinkssss = list(np.zeros(15))#raw_graph_features\n",
    "                    error_here = 1;\n",
    "                # hyperlink_data.append(datalinkssss)\n",
    "        else:\n",
    "            # hyperlink_data.append(list(np.zeros(15)))#raw_graph_features\n",
    "            error_here = 1;\n",
    "    \n",
    "    # except ConnectionError as e:\n",
    "    #     # print(\"No rep\", end = ',')\n",
    "    #     # hyperlink_data.append(list(np.zeros(15))) #raw_graph_features\n",
    "    #     error_here = 1; #v2\n",
    "    except Exception as e:\n",
    "        #logging.error(traceback.format_exc())\n",
    "        error_here = 1\n",
    "    \n",
    "    return (idx,  hyperlink_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ff57d-4c7d-452c-980a-9e64ab31812b",
   "metadata": {},
   "source": [
    "# Run HTML features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d84036c-da1d-4c9d-99e7-d35fd9548004",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Idea: Combine with manual features from HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1a612b8-8a9b-4092-bb92-edcdcf2e2b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML offline\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "# import whois\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "\n",
    "# Function to parse HTML content from a URL\n",
    "def get_html_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise error if the response is bad\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to extract HTML structure features\n",
    "def extract_structure_features(soup):\n",
    "    features = {}\n",
    "    \n",
    "    features['num_divs'] = len(soup.find_all('div'))\n",
    "    features['num_scripts'] = len(soup.find_all('script'))\n",
    "    features['num_forms'] = len(soup.find_all('form'))\n",
    "    features['num_links'] = len(soup.find_all('a'))\n",
    "    features['num_iframes'] = len(soup.find_all('iframe'))\n",
    "    \n",
    "    # Text to HTML ratio\n",
    "    text_length = len(soup.get_text())\n",
    "    html_length = len(str(soup))\n",
    "    features['text_to_html_ratio'] = text_length / html_length if html_length > 0 else 0\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Function to extract text-based features\n",
    "def extract_text_features(soup):\n",
    "    features = {}\n",
    "    \n",
    "    # Keywords in meta tags\n",
    "    meta_keywords = soup.find('meta', attrs={'name': 'keywords'})\n",
    "    #features['meta_keywords'] = meta_keywords['content'] if meta_keywords else ''\n",
    "    \n",
    "    # Title tag content\n",
    "    title = soup.title.string if soup.title else ''\n",
    "    features['title_length'] = len(title)\n",
    "    \n",
    "    # Visible text length\n",
    "    features['visible_text_length'] = len(soup.get_text())\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Function to extract link-based features\n",
    "def extract_link_features(soup, base_url):\n",
    "    features = {}\n",
    "    \n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    internal_links = [link for link in all_links if urlparse(link['href']).netloc == urlparse(base_url).netloc]\n",
    "    external_links = [link for link in all_links if urlparse(link['href']).netloc != urlparse(base_url).netloc]\n",
    "    \n",
    "    features['num_internal_links'] = len(internal_links)\n",
    "    features['num_external_links'] = len(external_links)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Function to extract JavaScript-based features\n",
    "def extract_js_features(soup):\n",
    "    features = {}\n",
    "    \n",
    "    script_tags = soup.find_all('script')\n",
    "    features['num_js_files'] = len(script_tags)\n",
    "    \n",
    "    # Suspicious JavaScript patterns\n",
    "    suspicious_js_patterns = ['eval', 'document.write', 'window.location']\n",
    "    features['suspicious_js_count'] = sum(\n",
    "        any(p in script.get_text() for p in suspicious_js_patterns) for script in script_tags)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Function to extract all features from a URL\n",
    "def extract_features(url, rec_id):\n",
    "#     html_content = get_html_content(url)\n",
    "#     if not html_content:\n",
    "#         return None\n",
    "    \n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    f = codecs.open(Path(data_dir + '/' + generate_filename(rec_id)), 'r', encoding='utf-8', errors='ignore')\n",
    "    soup = BeautifulSoup(f)\n",
    "    \n",
    "    # Extract all feature sets\n",
    "    structure_features = extract_structure_features(soup)\n",
    "    text_features = extract_text_features(soup)\n",
    "    link_features = extract_link_features(soup, url)\n",
    "    js_features = extract_js_features(soup)\n",
    "    # domain_features = extract_domain_features(url)\n",
    "    \n",
    "    # Combine all features into a single dictionary\n",
    "    features = {**structure_features, **text_features, **link_features, **js_features}\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dcb75cdb-89d8-47e3-9506-d77190356f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example URLs for feature extraction\n",
    "# urls = [\n",
    "#     \"https://dantri.com.vn\"\n",
    "# ]\n",
    "\n",
    "# # Extract features from each URL\n",
    "# features_list = []\n",
    "# for url in urls:\n",
    "#     features = extract_features(url)\n",
    "#     if features:\n",
    "#         features_list.append(features)\n",
    "\n",
    "# # Print extracted features\n",
    "# for idx, features in enumerate(features_list):\n",
    "#     print(f\"Features for URL {urls[idx]}:\\n\", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "50fca286-8617-4634-8abb-f5e9ca4f476c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[158, 17, 1, 129, 0, 0.049341527655838456, 5, 9554, 0, 129, 17, 0]\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "url = df_train.iloc[idx, 1]\n",
    "rec_id = smalldata.iloc[idx, 0]\n",
    "features = extract_features(url, rec_id)\n",
    "print(list(features.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "391d0dba-6dc5-49ff-8c74-53e543b487ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_html_features = []\n",
    "for idx in range(len(smalldata.index)):\n",
    "    url = smalldata.iloc[idx, 1]\n",
    "    rec_id = smalldata.iloc[idx, 0]\n",
    "    features = extract_features(url, rec_id)\n",
    "    manual_html_features.append(list(features.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c52a8f88-2428-405c-936c-bf0e8e588ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:  0 , fold:  0\n",
      "Train freq:  [151, 280]\n",
      "kNN, LightGBM, Run:  0 , fold:  1\n",
      "Train freq:  [141, 290]\n",
      "kNN, LightGBM, Run:  0 , fold:  2\n",
      "Train freq:  [150, 281]\n",
      "kNN, LightGBM, Run:  0 , fold:  3\n",
      "Train freq:  [140, 291]\n",
      "kNN, LightGBM, Run:  0 , fold:  4\n",
      "Train freq:  [146, 286]\n",
      "kNN, LightGBM, ['kNN', 'LightGBM']\n",
      "[0.55 0.64]\n"
     ]
    }
   ],
   "source": [
    "run_ML(np.array(manual_html_features), labels, \"Cuvation\", \"HTML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a3d6c70-e588-4a8c-be21-2c03f3d58f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2789b40f-fd91-48c2-a486-8bea298119ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d99a0af2-dbb5-4486-827e-6701f53505c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create dataset class for PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2e5afc86-02af-487e-88a4-ef99e60ee462",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: #don't delete this\n",
    "    graphs = []\n",
    "    labels_list = []\n",
    "    for i in range(len(smalldata.index)):\n",
    "    # for i in range(2):\n",
    "        nodes_list = []\n",
    "        attr_list = []\n",
    "        print(i, end = ',')\n",
    "        X, A, y, X_A, _ = get_udst_inputs(smalldata.iloc[i, 1], smalldata.iloc[i, 0], smalldata.iloc[i, 2])\n",
    "\n",
    "        child_id, source_id = A.nonzero()\n",
    "        edge_index = torch.tensor([source_id,\n",
    "                                   child_id], dtype=torch.long)\n",
    "        x = torch.tensor(X, dtype=torch.float)\n",
    "        y = torch.tensor([labels[i]], dtype=torch.int64)\n",
    "        data = Data(x=x, edge_index=edge_index, y = y)\n",
    "        graphs.append(data)\n",
    "        labels_list.append(labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23e24a6b-5a65-41f7-968a-5c7d01e6f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"graph_html\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(graphs, fp)\n",
    "# with open(\"graph_html_labels\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(labels_list, fp)\n",
    "    \n",
    "with open(\"graph_html\", \"rb\") as fp:   # Unpickling\n",
    "    graphs = pickle.load(fp)\n",
    "with open(\"graph_html_labels\", \"rb\") as fp:   # Unpickling\n",
    "    labels_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "989171c6-277c-434e-83a9-b618267e54a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train freq:  [539]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train freq: \", [len(list(group)) for key, group in groupby(sorted(labels_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95aeabfb-454b-4984-a875-a6104537382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "class GraphClassificationDataset(Dataset):\n",
    "    def __init__(self, graphs):\n",
    "        self.graphs = graphs\n",
    "        # self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        graph = self.graphs[idx]\n",
    "        # label = self.labels[idx]\n",
    "        return graph\n",
    "    \n",
    "    def get(): pass\n",
    "\n",
    "    def len(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e607ec88-7a07-476f-a644-96c494bed7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GraphClassificationDataset(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56107391-7b84-4614-b367-81d5d04b0545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: GraphClassificationDataset(539):\n",
      "====================\n",
      "Number of graphs: 539\n",
      "Number of features: 2\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa478edc-48bf-410f-b194-3fd09b43d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.shuffle()\n",
    "n_samples = len(dataset)\n",
    "# np.random.seed(0) \n",
    "# train_idx = list(np.random.choice(list(range(n_samples)), int(0.8*n_samples), replace=False))\n",
    "# test_idx = list(set(list(range(n_samples))).difference(set(train_idx)))\n",
    "# train_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c34344fc-0487-4632-8e65-676e88e892c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = dataset[:int(0.8*n_samples)]\n",
    "# test_dataset = dataset[int(0.8*n_samples):]\n",
    "train_dataset = [dataset[idx] for idx in train_idx]\n",
    "test_dataset = [dataset[idx] for idx in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb440b6e-47cc-4b2e-a554-e6ff6fec2bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(402, 137)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acf1c066-f7b9-4f66-8260-330c70c40068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729a4ea-1ac8-4ea5-97ef-8fedbbf3311a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build and train PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b7fba98-98b6-4a62-8917-8a5d10f2d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "        self.linconcat = Linear(2*hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a874e18f-2693-4626-a66c-15952318b86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train F1: 0.3982, Test F1: 0.3991\n",
      "Epoch: 010, Train F1: 0.3982, Test F1: 0.3991\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        # print(data.x.shape)\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    # for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "    #     out = model(data.x, data.edge_index, data.batch)  \n",
    "    #     pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    #     correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    # return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        true_labels += data.y.tolist()\n",
    "        pred_labels += pred.tolist()\n",
    "        # print(pred_labels)\n",
    "    return f1_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "for epoch in range(0, 20):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train F1: {train_acc:.4f}, Test F1: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef9e0c9b-e798-47c4-89c4-f16e0053e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['kNN', 'LightGBM'] min, max, avg of child features [0.82 0.92]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f69e8d-74fc-4369-aabb-e8d4b723a1ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GNN for dim. reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00edafd-7277-4644-b563-2910d441a5ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Net 1: 0.87, 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c74a0be3-48cf-4b24-9c56-48826ab199d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "class GCNdimReduce(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNdimReduce, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def dimReduce(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        # x = x.relu()\n",
    "        # x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc9cc7d-4f1a-476f-ab19-2643c4a404e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Net 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a751eb5f-e5eb-4450-8ba0-c65ef45d9486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "class GCNdimReduceV2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNdimReduceV2, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "       \n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def dimReduce(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717a4558-f031-44f1-9a1d-4780580cae3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bcd6b977-ad35-43eb-adbc-d401261e6d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train F1: 0.3982, Test F1: 0.3991\n",
      "Epoch: 001, Train F1: 0.3982, Test F1: 0.3991\n",
      "Epoch: 002, Train F1: 0.3982, Test F1: 0.3991\n",
      "Epoch: 003, Train F1: 0.3982, Test F1: 0.3991\n",
      "Epoch: 004, Train F1: 0.3982, Test F1: 0.3991\n",
      "Epoch: 005, Train F1: 0.3982, Test F1: 0.3991\n",
      "Epoch: 006, Train F1: 0.3982, Test F1: 0.3991\n",
      "Epoch: 007, Train F1: 0.3982, Test F1: 0.3991\n",
      "Epoch: 008, Train F1: 0.3982, Test F1: 0.3991\n",
      "Epoch: 009, Train F1: 0.3982, Test F1: 0.3991\n"
     ]
    }
   ],
   "source": [
    "n_hidden_channels = 16\n",
    "model = GCNdimReduce(hidden_channels=n_hidden_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        # print(data.x.shape)\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        true_labels += data.y.tolist()\n",
    "        pred_labels += pred.tolist()\n",
    "    return f1_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train F1: {train_acc:.4f}, Test F1: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a05648-8619-48c4-9f1f-32100030cb2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d717d58e-e981-4666-a8a5-703d1cc0eecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phish_url_vectorizer = []\n",
    "for link in list(smalldata.iloc[:,1]):\n",
    "    url_features = extract_feature_CountVectorizer(cv, link)\n",
    "    phish_url_vectorizer.append(list(url_features))\n",
    "# run_ML(concatGNN, labels, \"URLdatasetX2\", \"concatGNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b89c35c1-ac25-430e-b212-87492087a072",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(all_data_loader, train_loader, test_loader, n_hidden_channels = 16, n_epoch=1, lr=0.001):\n",
    "    # n_hidden_channels = 16\n",
    "    model = GCNdimReduce(hidden_channels=n_hidden_channels)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            # print(data.x.shape)\n",
    "            out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "            loss = criterion(out, data.y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            out = model(data.x, data.edge_index, data.batch)  \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "            true_labels += data.y.tolist()\n",
    "            pred_labels += pred.tolist()\n",
    "        return f1_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "    for epoch in range(0, n_epoch):\n",
    "        train()\n",
    "        train_acc = test(train_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Train F1: {train_acc:.4f}, Test F1: {test_acc:.4f}')\n",
    "    \n",
    "    model.eval()\n",
    "    dim_vec = torch.empty((0, n_hidden_channels), dtype=torch.float32)\n",
    "    for data in all_data_loader:\n",
    "        dim_x = model.dimReduce(data.x, data.edge_index, data.batch)\n",
    "        dim_vec = torch.cat((dim_vec, dim_x), 0)\n",
    "    return (dim_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac5bad79-d5d8-46ea-9935-52c6050c7a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train F1: 0.3982, Test F1: 0.3991\n"
     ]
    }
   ],
   "source": [
    "all_data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "dim_vec = train_model(all_data_loader, train_loader, test_loader)\n",
    "# model.eval()\n",
    "# dim_vec = torch.empty((0, n_hidden_channels), dtype=torch.float32)\n",
    "# # data = next(iter(test_loader))\n",
    "# for data in all_data_loader:\n",
    "#     dim_x = model.dimReduce(data.x, data.edge_index, data.batch)\n",
    "#     dim_vec = torch.cat((dim_vec, dim_x), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "56398e47-ab9e-42ba-b251-c5e26c2afc05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Combine hidden features with URL features\n",
    "# concatGNN = np.concatenate((np.array(phish_url_vectorizer), dim_vec.detach().numpy()),axis=1)\n",
    "# model_lgb = lgb.LGBMClassifier(verbose=-1)\n",
    "# model_lgb.fit(concatGNN[train_idx], labels[train_idx])\n",
    "# y_predict=model_lgb.predict(concatGNN[test_idx]) \n",
    "# print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7829d36e-41b5-4494-8ba3-4acd1a5f319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use hidden feature for testing\n",
    "# model_lgb = lgb.LGBMClassifier(verbose=-1)\n",
    "# graph_embedding = dim_vec.detach().numpy()\n",
    "# model_lgb.fit(graph_embedding[train_idx], labels[train_idx])\n",
    "# y_predict=model_lgb.predict(graph_embedding[test_idx]) \n",
    "# print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b41a3bad-f3f3-4106-95e0-fc589e386244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphs[0].x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b3ee4c5b-7b59-449a-8a2c-f6e9d90faf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats graph feature\n",
    "n_features_counter = int(graphs[0].x.shape[1]);\n",
    "html_hyperlink_features = np.zeros((smalldata.shape[0], 3*n_features_counter))\n",
    "for idx in range(len(graphs)):\n",
    "    # print(idx, hyper_np)\n",
    "    hyper_np = np.array(graphs[idx].x)\n",
    "    if hyper_np.shape[0] >= 2:\n",
    "        html_hyperlink_features[idx, :] = np.hstack((hyper_np.min(axis=0),hyper_np.max(axis=0), hyper_np.mean(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ed1f3fb-1dc7-4a9a-8970-fb172b6ff437",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatGNN_graph = np.concatenate((np.array(phish_url_vectorizer), html_hyperlink_features),axis=1)\n",
    "# concatGNN_graph = np.array(phish_url_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0573095a-0b98-426a-9306-484f56dd2404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "model_lgb2 = lgb.LGBMClassifier(verbose=-1)\n",
    "model_lgb2.fit(concatGNN_graph[train_idx], labels[train_idx])\n",
    "y_predict=model_lgb2.predict(concatGNN_graph[test_idx]) \n",
    "print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2049395f-22e0-48c2-8992-3e9c1638f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGRAPHISH(train_idx, test_idx):\n",
    "    stack_GNNs_graph = np.concatenate((np.array(phish_url_vectorizer), html_hyperlink_features),axis=1)\n",
    "    # stack_GNNs_graph = np.array(phish_url_vectorizer)\n",
    "    for i in range(10):\n",
    "        np.random.seed(i) \n",
    "        # train_idx_new = list(np.random.choice(list(range(n_samples)), int(0.8*n_samples), replace=False))\n",
    "        # train_idx_new = list(set(train_idx_new).difference(test_idx))\n",
    "        train_idx_new = list(np.random.choice(train_idx, int(0.8*len(train_idx)), replace=False))\n",
    "        print(train_idx_new[:5])\n",
    "        train_dataset_new = [dataset[idx] for idx in train_idx_new]\n",
    "        train_loader_new = DataLoader(train_dataset_new, batch_size=8, shuffle=True)\n",
    "        n_hidden_channels = 2\n",
    "        n_epoch = 1\n",
    "        dim_vec_new = train_model(all_data_loader, train_loader_new, test_loader, n_hidden_channels, n_epoch)\n",
    "        stack_GNNs_graph = np.concatenate((stack_GNNs_graph, dim_vec_new.detach().numpy()),axis=1)\n",
    "    # Le: compute new method\n",
    "    \n",
    "    \n",
    "    model_lgb2 = lgb.LGBMClassifier(verbose=-1)\n",
    "    model_lgb2.fit(stack_GNNs_graph[train_idx], labels[train_idx])\n",
    "    y_predict=model_lgb2.predict(stack_GNNs_graph[test_idx]) \n",
    "    y_proba=model_lgb2.predict_proba(stack_GNNs_graph[test_idx])[:,1]\n",
    "    print(f1_score(y_predict, labels[test_idx], average='macro'))\n",
    "    df_results = pd.DataFrame({'true_label': labels[test_idx], 'pred_label': y_predict, 'predict_proba': y_proba})\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68f508be-1c0e-4493-bac1-69bb3f3f792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199, 397, 59, 78, 65]\n",
      "Epoch: 000, Train F1: 0.3977, Test F1: 0.3991\n",
      "[162, 300, 400, 117, 80]\n",
      "Epoch: 000, Train F1: 0.3989, Test F1: 0.3991\n",
      "[25, 337, 53, 100, 190]\n",
      "Epoch: 000, Train F1: 0.4067, Test F1: 0.3991\n",
      "[248, 320, 81, 255, 217]\n",
      "Epoch: 000, Train F1: 0.3955, Test F1: 0.3991\n",
      "[184, 6, 61, 312, 100]\n",
      "Epoch: 000, Train F1: 0.3977, Test F1: 0.3991\n",
      "[52, 123, 282, 71, 222]\n",
      "Epoch: 000, Train F1: 0.4000, Test F1: 0.3991\n",
      "[53, 392, 61, 13, 373]\n",
      "Epoch: 000, Train F1: 0.4067, Test F1: 0.3991\n",
      "[248, 65, 120, 132, 380]\n",
      "Epoch: 000, Train F1: 0.3989, Test F1: 0.3991\n",
      "[59, 210, 97, 305, 194]\n",
      "Epoch: 000, Train F1: 0.4045, Test F1: 0.3991\n",
      "[57, 32, 17, 167, 2]\n",
      "Epoch: 000, Train F1: 0.3955, Test F1: 0.3991\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>predict_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     true_label  pred_label  predict_proba\n",
       "0             1           1       0.999985\n",
       "1             1           1       0.999985\n",
       "2             1           1       0.999985\n",
       "3             1           1       0.999985\n",
       "4             1           1       0.999985\n",
       "..          ...         ...            ...\n",
       "132           1           1       0.999985\n",
       "133           1           1       0.999985\n",
       "134           1           1       0.999983\n",
       "135           1           1       0.999979\n",
       "136           1           1       0.999985\n",
       "\n",
       "[137 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runGRAPHISH(train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fe59b5-c7f8-4fca-82c3-bd2a162b0b73",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hyperlinks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e3459ba-c3fe-4178-8115-3984b611e23e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()\n",
    "n_cores = min(30, int(multiprocessing.cpu_count()-2))\n",
    "n_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b8652ab-ab89-4a6d-bfcb-3c0c548db9bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'html_4000_f'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_name_0123 = data_dir.split('/')[-1][:-4]\n",
    "n_subsample = 100\n",
    "data_name_0123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "766cd5a5-dcb4-4f03-a0ed-ee9d3ba25d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exist! Load the data\n"
     ]
    }
   ],
   "source": [
    "data_name_0123 = data_dir.split('/')[-1][:-4]\n",
    "# data_file = \"data/raw_graph_features_v2.pickle\" # first version \n",
    "data_file = \"data/\"+data_name_0123+str(n_subsample)+\"v2_raw_graph_features_CountVectorizer.pickle\" # first version \n",
    "my_file = Path(data_file)\n",
    "if my_file.is_file():\n",
    "    print(\"File exist! Load the data\")\n",
    "    with open(data_file, \"rb\") as fp:   # Unpickling\n",
    "        results = pickle.load(fp)\n",
    "else:\n",
    "    print(\"File does not exist! Process the data\")\n",
    "    n_test_samples = int(smalldata.shape[0]) # how many link we want to test\n",
    "    from joblib import Parallel, delayed\n",
    "    results = Parallel(n_jobs=n_cores)(delayed(get_graph_features_CountVectorizer)(i) for i in range(n_test_samples)) # test on 100 links\n",
    "    with open(data_file, \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3bee1e3a-efd0-41a0-8c14-ed23c50940a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/html_4000_f100v2_raw_graph_features_CountVectorizer.pickle'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "71158727-28ae-4c6d-aa68-f8430da884a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be4688-e4d2-4401-917c-31bf7ce340b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create dataset class for PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16ccaf55-e1fa-46b7-9f1b-c521065a1798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have a list of graphs represented as Data objects and a corresponding list of labels\n",
    "hyperlink_graphs = []\n",
    "hyperlink_labels_list = []\n",
    "for i in range(len(results)):\n",
    "    idx, graph_feature = results[i]\n",
    "    n_hyperlinks = len(graph_feature)-1\n",
    "    child_id = [i+1 for i in range(n_hyperlinks)]\n",
    "    source_id = list(np.zeros(n_hyperlinks).astype(int))\n",
    "    # edges are connecting node 0 to nodes 1, 2, .., n_hpyterlinks\n",
    "    edge_index = torch.tensor([source_id,\n",
    "                               child_id], dtype=torch.long)\n",
    "    x = torch.tensor(graph_feature, dtype=torch.float)\n",
    "    y = torch.tensor([labels[idx]], dtype=torch.int64)\n",
    "    data = Data(x=x, edge_index=edge_index, y = y)\n",
    "    if n_hyperlinks > -1:\n",
    "        hyperlink_graphs.append(data)\n",
    "        hyperlink_labels_list.append(labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "795e7cc1-6f35-4ed2-863a-34cece9a52da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train freq:  [182, 357]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train freq: \", [len(list(group)) for key, group in groupby(sorted(hyperlink_labels_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "44a3a5ee-7d6e-412c-8ec3-7a653bbbb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlink_dataset = GraphClassificationDataset(hyperlink_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6dc5a361-8c19-48a4-a651-f46ca40ffdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: GraphClassificationDataset(539):\n",
      "====================\n",
      "Number of graphs: 539\n",
      "Number of features: 369\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {hyperlink_dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(hyperlink_dataset)}')\n",
    "print(f'Number of features: {hyperlink_dataset.num_features}')\n",
    "print(f'Number of classes: {hyperlink_dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e69f8dd0-7d89-4d23-9051-1e86c544d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.shuffle()\n",
    "n_samples = len(hyperlink_dataset)\n",
    "# np.random.seed(0) \n",
    "# train_idx = list(np.random.choice(list(range(n_samples)), int(0.8*n_samples), replace=False))\n",
    "# test_idx = list(set(list(range(n_samples))).difference(set(train_idx)))\n",
    "# train_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "def48b62-971c-4e4f-ac9c-126a06f2739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = dataset[:int(0.8*n_samples)]\n",
    "# test_dataset = dataset[int(0.8*n_samples):]\n",
    "hyperlink_train_dataset = [hyperlink_dataset[idx] for idx in train_idx]\n",
    "hyperlink_test_dataset = [hyperlink_dataset[idx] for idx in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b8c74ac-9ca9-498f-b5dc-ee77ab28f254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(402, 137)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hyperlink_train_dataset), len(hyperlink_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f115489a-ccce-4f40-9355-452d4cc6b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "hyperlink_train_loader = DataLoader(hyperlink_train_dataset, batch_size=16, shuffle=True)\n",
    "hyperlink_test_loader = DataLoader(hyperlink_test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc21d4-1976-46ce-8640-d6994adafd0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Build and train PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5f975f01-7eb2-4b3e-9bd7-b8c897785e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.nn import Linear\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import GCNConv\n",
    "# from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, hidden_channels):\n",
    "#         super(GCN, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "#         self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "#         self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "#         self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "#         self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "#         self.linconcat = Linear(2*hidden_channels, dataset.num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         # 1. Obtain node embeddings \n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = x.relu()\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = x.relu()\n",
    "#         x = self.conv3(x, edge_index)\n",
    "\n",
    "#         # 2. Readout layer\n",
    "#         x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "#         # 3. Apply a final classifier\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.lin(x)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd41cc6b-7c71-4d42-9b24-5419f05f98c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GCN(hidden_channels=64)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# def train():\n",
    "#     model.train()\n",
    "\n",
    "#     for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "#         # print(data.x.shape)\n",
    "#         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "#         loss = criterion(out, data.y)  # Compute the loss.\n",
    "#         loss.backward()  # Derive gradients.\n",
    "#         optimizer.step()  # Update parameters based on gradients.\n",
    "#         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "# def test(loader):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     # for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "#     #     out = model(data.x, data.edge_index, data.batch)  \n",
    "#     #     pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "#     #     correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "#     # return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "#     true_labels = []\n",
    "#     pred_labels = []\n",
    "#     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "#         out = model(data.x, data.edge_index, data.batch)  \n",
    "#         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "#         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "#         true_labels += data.y.tolist()\n",
    "#         pred_labels += pred.tolist()\n",
    "#         # print(pred_labels)\n",
    "#     return f1_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "# for epoch in range(0, 20):\n",
    "#     train()\n",
    "#     train_acc = test(train_loader)\n",
    "#     test_acc = test(test_loader)\n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f'Epoch: {epoch:03d}, Train F1: {train_acc:.4f}, Test F1: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9bb46af3-23a3-4c2b-a73a-496e34bdd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['kNN', 'LightGBM'] min, max, avg of child features [0.82 0.92]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7128b95-c87e-4614-b2b6-65b372846199",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GNN for dim. reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d0b6c-94b6-46fe-ae04-8a22b8244a20",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Net 1: 0.87, 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "93e113dd-b47e-4696-854f-6cd2bf99f479",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "class hyperlink_GCNdimReduce(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(hyperlink_GCNdimReduce, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(hyperlink_dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hyperlink_dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def dimReduce(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        # x = x.relu()\n",
    "        # x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b201f-f9cd-4c2c-8fb2-299019c33656",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Net 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "49cab21a-1649-4cfb-8b32-775372f8855e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "class hyperlink_GCNdimReduceV2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(hyperlink_GCNdimReduceV2, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(hyperlink_dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hyperlink_dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "       \n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def dimReduce(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4c5b2-1bbe-4de7-b566-e55cddcae716",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75e36e2e-3075-4846-9958-66eafd46a08a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train F1: 0.3982, Test F1: 0.3991\n",
      "Epoch: 001, Train F1: 0.5324, Test F1: 0.4863\n",
      "Epoch: 002, Train F1: 0.8981, Test F1: 0.9338\n",
      "Epoch: 003, Train F1: 0.9307, Test F1: 0.9330\n",
      "Epoch: 004, Train F1: 0.9457, Test F1: 0.9418\n",
      "Epoch: 005, Train F1: 0.9671, Test F1: 0.9418\n",
      "Epoch: 006, Train F1: 0.9539, Test F1: 0.9589\n",
      "Epoch: 007, Train F1: 0.9862, Test F1: 0.9418\n",
      "Epoch: 008, Train F1: 0.9889, Test F1: 0.9153\n",
      "Epoch: 009, Train F1: 0.9861, Test F1: 0.9062\n"
     ]
    }
   ],
   "source": [
    "n_hidden_channels = 16\n",
    "hyperlink_model = hyperlink_GCNdimReduce(hidden_channels=n_hidden_channels)\n",
    "optimizer = torch.optim.Adam(hyperlink_model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    hyperlink_model.train()\n",
    "\n",
    "    for data in hyperlink_train_loader:  # Iterate in batches over the training dataset.\n",
    "        # print(data.x.shape)\n",
    "        out = hyperlink_model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "    hyperlink_model.eval()\n",
    "    correct = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = hyperlink_model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        true_labels += data.y.tolist()\n",
    "        pred_labels += pred.tolist()\n",
    "    return f1_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    train()\n",
    "    train_acc = test(hyperlink_train_loader)\n",
    "    test_acc = test(hyperlink_test_loader)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train F1: {train_acc:.4f}, Test F1: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd05c57-15eb-4dff-8214-e14c1e64ead7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6c089665-699b-4361-abc9-90a5add2357f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phish_url_vectorizer = []\n",
    "for link in list(smalldata.iloc[:,1]):\n",
    "    url_features = extract_feature_CountVectorizer(cv, link)\n",
    "    phish_url_vectorizer.append(list(url_features))\n",
    "# run_ML(concatGNN, labels, \"URLdatasetX2\", \"concatGNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "09e0211a-4123-4693-a978-83c7e103a693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_hyperlink_model(all_data_loader, train_loader, test_loader, n_hidden_channels = 16, n_epoch=1, lr=0.001):\n",
    "    # n_hidden_channels = 16\n",
    "    model = hyperlink_GCNdimReduce(hidden_channels=n_hidden_channels)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            # print(data.x.shape)\n",
    "            out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "            loss = criterion(out, data.y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            out = model(data.x, data.edge_index, data.batch)  \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "            true_labels += data.y.tolist()\n",
    "            pred_labels += pred.tolist()\n",
    "        return f1_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "    for epoch in range(0, n_epoch):\n",
    "        train()\n",
    "        train_acc = test(train_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Train F1: {train_acc:.4f}, Test F1: {test_acc:.4f}')\n",
    "    \n",
    "    model.eval()\n",
    "    dim_vec = torch.empty((0, n_hidden_channels), dtype=torch.float32)\n",
    "    for data in all_data_loader:\n",
    "        dim_x = model.dimReduce(data.x, data.edge_index, data.batch)\n",
    "        dim_vec = torch.cat((dim_vec, dim_x), 0)\n",
    "    return (dim_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2d3201ea-2ee2-40ca-8d89-8dfbf13896d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train F1: 0.3982, Test F1: 0.3991\n"
     ]
    }
   ],
   "source": [
    "hyperlink_all_data_loader = DataLoader(hyperlink_dataset, batch_size=1, shuffle=False)\n",
    "hyperlink_dim_vec = train_hyperlink_model(hyperlink_all_data_loader, hyperlink_train_loader, hyperlink_test_loader)\n",
    "# model.eval()\n",
    "# dim_vec = torch.empty((0, n_hidden_channels), dtype=torch.float32)\n",
    "# # data = next(iter(test_loader))\n",
    "# for data in all_data_loader:\n",
    "#     dim_x = model.dimReduce(data.x, data.edge_index, data.batch)\n",
    "#     dim_vec = torch.cat((dim_vec, dim_x), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2d73d8c8-fbb4-4aad-86de-173e4a05532e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatGNN = np.concatenate((np.array(phish_url_vectorizer), hyperlink_dim_vec.detach().numpy()),axis=1)\n",
    "# model_lgb = lgb.LGBMClassifier(verbose=-1)\n",
    "# model_lgb.fit(concatGNN[train_idx], labels[train_idx])\n",
    "# y_predict=model_lgb.predict(concatGNN[test_idx]) \n",
    "# print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f4b8358b-0dd5-4200-90a6-3354ba9a4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lgb = lgb.LGBMClassifier(verbose=-1)\n",
    "# graph_embedding = hyperlink_dim_vec.detach().numpy()\n",
    "# model_lgb.fit(graph_embedding[train_idx], labels[train_idx])\n",
    "# y_predict=model_lgb.predict(graph_embedding[test_idx]) \n",
    "# print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9c1876a8-77f4-4055-8132-db0c6a1a3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats graph feature\n",
    "idx, vec = results[0]; vec = np.array(vec); n_features_counter = int(vec.shape[1]);\n",
    "hyperlink_features = np.zeros((smalldata.shape[0], 3*n_features_counter))\n",
    "for idx, hyper_np in results:\n",
    "    # print(idx, hyper_np)\n",
    "    hyper_np = np.array(hyper_np)\n",
    "    if hyper_np.shape[0] >= 2:\n",
    "        hyperlink_features[idx, :] = np.hstack((hyper_np.min(axis=0),hyper_np.max(axis=0), hyper_np.mean(axis=0)))\n",
    "    # hyperlink_features[idx, :] = hyper_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1469d86b-66ca-4b2b-9ceb-b6c7563658aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatGNN_graph = np.concatenate((np.array(phish_url_vectorizer), hyperlink_features),axis=1)\n",
    "# concatGNN_graph = np.concatenate((np.array(phish_url_vectorizer), hyperlink_features,  dim_vec.detach().numpy()),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2f26d260-c6b8-4c0d-bbe3-c633b4f4c618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "model_lgb2 = lgb.LGBMClassifier(verbose=-1)\n",
    "model_lgb2.fit(concatGNN_graph[train_idx], labels[train_idx])\n",
    "y_predict=model_lgb2.predict(concatGNN_graph[test_idx]) \n",
    "print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1f23fe99-71bf-4223-b228-d8052843ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runHyperlinkGRAPHISH(train_idx, test_idx):\n",
    "    stack_GNNs_graph = np.concatenate((np.array(phish_url_vectorizer), hyperlink_features),axis=1)\n",
    "    for i in range(10):\n",
    "        np.random.seed(i) \n",
    "        # train_idx_new = list(np.random.choice(list(range(n_samples)), int(0.8*n_samples), replace=False))\n",
    "        # train_idx_new = list(set(train_idx_new).difference(test_idx))\n",
    "        train_idx_new = list(np.random.choice(train_idx, int(0.8*len(train_idx)), replace=False))\n",
    "        print(train_idx_new[:5])\n",
    "        train_dataset_new = [hyperlink_dataset[idx] for idx in train_idx_new]\n",
    "        train_loader_new = DataLoader(train_dataset_new, batch_size=8, shuffle=True)\n",
    "        n_hidden_channels = 2\n",
    "        n_epoch = 1\n",
    "        dim_vec_new = train_hyperlink_model(hyperlink_all_data_loader, train_loader_new, hyperlink_test_loader, n_hidden_channels, n_epoch)\n",
    "        stack_GNNs_graph = np.concatenate((stack_GNNs_graph, dim_vec_new.detach().numpy()),axis=1)\n",
    "    # Le: compute new method\n",
    "    \n",
    "    \n",
    "    model_lgb2 = lgb.LGBMClassifier(verbose=-1)\n",
    "    model_lgb2.fit(stack_GNNs_graph[train_idx], labels[train_idx])\n",
    "    y_predict=model_lgb2.predict(stack_GNNs_graph[test_idx]) \n",
    "    y_proba=model_lgb2.predict_proba(stack_GNNs_graph[test_idx])[:,1]\n",
    "    print(f1_score(y_predict, labels[test_idx], average='macro'))\n",
    "    df_results = pd.DataFrame({'true_label': labels[test_idx], 'pred_label': y_predict, 'predict_proba': y_proba})\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dff13f31-d4dc-4eda-8bb0-23a9c449b2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199, 397, 59, 78, 65]\n",
      "Epoch: 000, Train F1: 0.7186, Test F1: 0.8737\n",
      "[162, 300, 400, 117, 80]\n",
      "Epoch: 000, Train F1: 0.3360, Test F1: 0.4439\n",
      "[25, 337, 53, 100, 190]\n",
      "Epoch: 000, Train F1: 0.5794, Test F1: 0.8188\n",
      "[248, 320, 81, 255, 217]\n",
      "Epoch: 000, Train F1: 0.7188, Test F1: 0.9130\n",
      "[184, 6, 61, 312, 100]\n",
      "Epoch: 000, Train F1: 0.6859, Test F1: 0.8981\n",
      "[52, 123, 282, 71, 222]\n",
      "Epoch: 000, Train F1: 0.4950, Test F1: 0.6194\n",
      "[53, 392, 61, 13, 373]\n",
      "Epoch: 000, Train F1: 0.2934, Test F1: 0.4804\n",
      "[248, 65, 120, 132, 380]\n",
      "Epoch: 000, Train F1: 0.6955, Test F1: 0.8981\n",
      "[59, 210, 97, 305, 194]\n",
      "Epoch: 000, Train F1: 0.6735, Test F1: 0.8907\n",
      "[57, 32, 17, 167, 2]\n",
      "Epoch: 000, Train F1: 0.4266, Test F1: 0.5486\n",
      "0.991773254068336\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>predict_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     true_label  pred_label  predict_proba\n",
       "0             1           1       0.999982\n",
       "1             1           1       0.999979\n",
       "2             1           1       0.999968\n",
       "3             1           1       0.999985\n",
       "4             1           1       0.999982\n",
       "..          ...         ...            ...\n",
       "132           1           1       0.999964\n",
       "133           1           1       0.999983\n",
       "134           1           1       0.999942\n",
       "135           1           1       0.999983\n",
       "136           1           1       0.999973\n",
       "\n",
       "[137 rows x 3 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runHyperlinkGRAPHISH(train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "309c61c6-f8fa-4c06-999f-576892f31bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runHTMLHyperlinkGRAPHISH(train_idx, test_idx):\n",
    "    stack_GNNs_graph = np.concatenate((np.array(phish_url_vectorizer), hyperlink_features),axis=1)\n",
    "    for i in range(10):\n",
    "        np.random.seed(i) \n",
    "        # train_idx_new = list(np.random.choice(list(range(n_samples)), int(0.8*n_samples), replace=False))\n",
    "        # train_idx_new = list(set(train_idx_new).difference(test_idx))\n",
    "        train_idx_new = list(np.random.choice(train_idx, int(0.8*len(train_idx)), replace=False))\n",
    "        print(train_idx_new[:5])\n",
    "        train_dataset_new = [hyperlink_dataset[idx] for idx in train_idx_new]\n",
    "        train_loader_new = DataLoader(train_dataset_new, batch_size=8, shuffle=True)\n",
    "        n_hidden_channels = 2\n",
    "        n_epoch = 1\n",
    "        dim_vec_new = train_hyperlink_model(hyperlink_all_data_loader, train_loader_new, hyperlink_test_loader, n_hidden_channels, n_epoch)\n",
    "        stack_GNNs_graph = np.concatenate((stack_GNNs_graph, dim_vec_new.detach().numpy()),axis=1)\n",
    "    # HTML: compute new method\n",
    "    for i in range(10):\n",
    "        np.random.seed(i) \n",
    "        # train_idx_new = list(np.random.choice(list(range(n_samples)), int(0.8*n_samples), replace=False))\n",
    "        # train_idx_new = list(set(train_idx_new).difference(test_idx))\n",
    "        train_idx_new = list(np.random.choice(train_idx, int(0.8*len(train_idx)), replace=False))\n",
    "        print(train_idx_new[:5])\n",
    "        train_dataset_new = [dataset[idx] for idx in train_idx_new]\n",
    "        train_loader_new = DataLoader(train_dataset_new, batch_size=8, shuffle=True)\n",
    "        n_hidden_channels = 2\n",
    "        n_epoch = 1\n",
    "        dim_vec_new = train_model(all_data_loader, train_loader_new, test_loader, n_hidden_channels, n_epoch)\n",
    "        stack_GNNs_graph = np.concatenate((stack_GNNs_graph, dim_vec_new.detach().numpy()),axis=1)\n",
    "    \n",
    "    model_lgb2 = lgb.LGBMClassifier(verbose=-1)\n",
    "    model_lgb2.fit(stack_GNNs_graph[train_idx], labels[train_idx])\n",
    "    y_predict=model_lgb2.predict(stack_GNNs_graph[test_idx]) \n",
    "    y_proba=model_lgb2.predict_proba(stack_GNNs_graph[test_idx])[:,1]\n",
    "    print(f1_score(y_predict, labels[test_idx], average='macro'))\n",
    "    df_results = pd.DataFrame({'true_label': labels[test_idx], 'pred_label': y_predict, 'predict_proba': y_proba})\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c622dfbd-432c-4d04-9226-9d80fc03684d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199, 397, 59, 78, 65]\n",
      "Epoch: 000, Train F1: 0.7186, Test F1: 0.8737\n",
      "[162, 300, 400, 117, 80]\n",
      "Epoch: 000, Train F1: 0.3360, Test F1: 0.4439\n",
      "[25, 337, 53, 100, 190]\n",
      "Epoch: 000, Train F1: 0.5794, Test F1: 0.8188\n",
      "[248, 320, 81, 255, 217]\n",
      "Epoch: 000, Train F1: 0.7188, Test F1: 0.9130\n",
      "[184, 6, 61, 312, 100]\n",
      "Epoch: 000, Train F1: 0.6859, Test F1: 0.8981\n",
      "[52, 123, 282, 71, 222]\n",
      "Epoch: 000, Train F1: 0.4950, Test F1: 0.6194\n",
      "[53, 392, 61, 13, 373]\n",
      "Epoch: 000, Train F1: 0.2934, Test F1: 0.4804\n",
      "[248, 65, 120, 132, 380]\n",
      "Epoch: 000, Train F1: 0.6955, Test F1: 0.8981\n",
      "[59, 210, 97, 305, 194]\n",
      "Epoch: 000, Train F1: 0.6735, Test F1: 0.8907\n",
      "[57, 32, 17, 167, 2]\n",
      "Epoch: 000, Train F1: 0.4266, Test F1: 0.5486\n",
      "[199, 397, 59, 78, 65]\n",
      "Epoch: 000, Train F1: 0.3977, Test F1: 0.3991\n",
      "[162, 300, 400, 117, 80]\n",
      "Epoch: 000, Train F1: 0.3989, Test F1: 0.3991\n",
      "[25, 337, 53, 100, 190]\n",
      "Epoch: 000, Train F1: 0.4067, Test F1: 0.3991\n",
      "[248, 320, 81, 255, 217]\n",
      "Epoch: 000, Train F1: 0.3955, Test F1: 0.3991\n",
      "[184, 6, 61, 312, 100]\n",
      "Epoch: 000, Train F1: 0.3977, Test F1: 0.3991\n",
      "[52, 123, 282, 71, 222]\n",
      "Epoch: 000, Train F1: 0.4000, Test F1: 0.3991\n",
      "[53, 392, 61, 13, 373]\n",
      "Epoch: 000, Train F1: 0.4067, Test F1: 0.3991\n",
      "[248, 65, 120, 132, 380]\n",
      "Epoch: 000, Train F1: 0.3989, Test F1: 0.3991\n",
      "[59, 210, 97, 305, 194]\n",
      "Epoch: 000, Train F1: 0.4045, Test F1: 0.3991\n",
      "[57, 32, 17, 167, 2]\n",
      "Epoch: 000, Train F1: 0.3955, Test F1: 0.3991\n",
      "0.991773254068336\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>predict_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     true_label  pred_label  predict_proba\n",
       "0             1           1       0.999974\n",
       "1             1           1       0.999975\n",
       "2             1           1       0.999978\n",
       "3             1           1       0.999985\n",
       "4             1           1       0.999975\n",
       "..          ...         ...            ...\n",
       "132           1           1       0.999979\n",
       "133           1           1       0.999982\n",
       "134           1           1       0.999947\n",
       "135           1           1       0.999980\n",
       "136           1           1       0.999979\n",
       "\n",
       "[137 rows x 3 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runHTMLHyperlinkGRAPHISH(train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0152895e-7e68-4621-a2fc-ed704efe239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set_save = 'Test_NewData'\n",
    "# n_loops = 1; n_folds = 5;\n",
    "# base_dir = 'comparision_results/html/test1'\n",
    "# approach = 'GRAPHISHS'\n",
    "# data_set = data_dir[5:-4]\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# for i in range(n_loops):\n",
    "#     cv = KFold(n_splits=n_folds, shuffle=True, random_state = i)\n",
    "#     # print(\"Use stratifired fold\")\n",
    "#     # cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state = i)\n",
    "#     for fold, (train_idx, test_idx) in enumerate(cv.split(phish_url_vectorizer)):\n",
    "#         path_dir = base_dir +'/' + data_set_save + '_run_'+str(i)+'_'+ 'fold_'+str(fold)+'_'+approach\n",
    "#         print('Run: ', i, ', fold: ', fold)\n",
    "#         # X_train = X[train_idx]\n",
    "#         # X_test = X[test_idx]\n",
    "#         # y_train = y[train_idx]\n",
    "#         # y_test = y[test_idx]\n",
    "#         df = runGRAPHISH(train_idx, test_idx)\n",
    "#         path_dir = base_dir +'/' + data_set_save + '_run_'+str(i)+'_'+ 'fold_'+str(fold)+'_'+approach\n",
    "#         df.to_csv(path_dir + '_GRAPHISH_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc0a0f-4bcf-47b0-9de5-1d6e06af4784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0ae5c4-9352-4260-ac08-0129f4105a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26acb339-fe13-4890-abe2-0e37e868bbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d65b4d9-0aca-4382-863f-f1be52b4610a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ddf39a-71da-4e65-a346-7afb77d89871",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [8, 9, 3, 4, 2]\n",
    "n = len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82c01e3-dfd1-49d1-8c6f-7902000866dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        if A[i] > A[j]:\n",
    "            # Doi cho A[i] va A[j]\n",
    "            temp = A[j]\n",
    "            A[j] = A[i]\n",
    "            A[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15f47d7-6838-41af-bd40-e3ae9e010ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 8, 9]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b6cab2-1777-4c5a-8cbf-496c99f04385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
