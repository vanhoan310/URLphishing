{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db6ea671-60fe-4cfe-ae8f-d7c37cbdebf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import run_ML\n",
    "from sklearn.metrics import f1_score\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9132d59-ef74-44d2-91aa-e89d9b5fc54d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51996577-859a-466e-a40f-b60f6a51a8d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ac7c73-a600-48e9-8931-3afa1679e506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tldextract\n",
    "# !pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c810e445-d6b5-4f8a-b972-d73a32432893",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_dir = \"data/URLdatasetX2_1.csv\"\n",
    "data_dir = \"data/URLdatasetX2_1sub10.csv\"\n",
    "df = pd.read_csv(data_dir,index_col=0)\n",
    "# n_subsample = 3000 # all\n",
    "# smalldata = df.sample(n = n_subsample, random_state=1) #PC\n",
    "n_subsample = 'full'; smalldata = df;\n",
    "# get labels of urls\n",
    "labels = smalldata.iloc[:,-1].values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18402ea2-1557-4bab-9803-de5f8eb50f27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# smalldata['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf2b33cf-d813-4532-933f-260f85c6e76b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_dir = \"data/malicious_phish_http_filter.csv\"\n",
    "# df = pd.read_csv(data_dir,index_col=0)\n",
    "# df = df.loc[df.type!='defacement']\n",
    "# # n_subsample = 3000 # all\n",
    "# # smalldata = df.sample(n = n_subsample, random_state=1) #PC\n",
    "# n_subsample = 'full'; smalldata = df;\n",
    "# # get labels of urls\n",
    "# labels = smalldata.iloc[:,-1].values\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# labels = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aca4692-f14f-4210-88cb-68aa5654bbc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # data_dir = \"data/LegitPhish_50_50.csv\"\n",
    "# # data_dir = \"data/LegitPhish_80_20.csv\"\n",
    "# data_dir = \"data/LegitPhish_90_10.csv\"\n",
    "# df = pd.read_csv(data_dir,index_col=0)\n",
    "# n_subsample = 3000 # all\n",
    "# smalldata = df.sample(n = n_subsample, random_state=1) #PC\n",
    "# # smalldata = df\n",
    "# # get labels of urls\n",
    "# labels = smalldata.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcd08658-b0b2-44af-8058-95c255eed904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 2029, 1: 225}), 2254)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "counter = collections.Counter(labels)\n",
    "counter, len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a837d-63e3-40cf-afc9-9593072d34e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Conventional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cd8d0e0-af10-42b5-9bd9-bced95415a22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "155e525e-f2fc-49d2-8be2-3b64ea9b67b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'domain': 'www.example.com', 'num_subdomains': 2, 'contains_ip': 0, 'path_length': 20, 'num_path_segments': 3, 'uses_https': 0, 'file_extension': 'html', 'count_special_characters': 11, 'count_non_alphanumeric_characters': 11, 'TLD': 'com', 'count_obfuscated_characters': 0, 'letter_ratio_in_url': 0.7380952380952381, 'digit_ratio_in_url': 0.0, 'count_equals_in_url': 2, 'NoOfAmpersandInURL': 0, 'CharContinuationRate': 0.11904761904761904, 'ratio_obfuscated_characters': 0.0, 'NoOfQMarkInURL': 0}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "url = \"http://www.example.com/path/to/==file.html\"\n",
    "url_features = extract_features(url)\n",
    "print(url_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2377259d-5b19-4f0f-8758-96b1b330d117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(url_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66cae4c3-a178-435c-9830-a4f5fd8fdc06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get numerical and catergorical features\n",
    "phish_url = []\n",
    "for link in list(smalldata.iloc[:,0]):\n",
    "    url_features = extract_features(link)\n",
    "    phish_url.append(list(url_features.values())[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "664a11ec-0a42-4a01-91f2-208aa5796b64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phish_url_df = pd.DataFrame(phish_url, columns = list(url_features.keys())[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "383162e7-d058-471e-b921-d2e983efe066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# phish_url_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aabba33a-d06a-4a3f-b021-ff8cbde0088c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phish_url_df.iloc[:,5] = pd.Categorical(phish_url_df.iloc[:,5]).codes\n",
    "phish_url_df.iloc[:,8] = pd.Categorical(phish_url_df.iloc[:,8]).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cc59152-00c2-41ec-acf5-afeae474f39e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_subdomains</th>\n",
       "      <th>contains_ip</th>\n",
       "      <th>path_length</th>\n",
       "      <th>num_path_segments</th>\n",
       "      <th>uses_https</th>\n",
       "      <th>file_extension</th>\n",
       "      <th>count_special_characters</th>\n",
       "      <th>count_non_alphanumeric_characters</th>\n",
       "      <th>TLD</th>\n",
       "      <th>count_obfuscated_characters</th>\n",
       "      <th>letter_ratio_in_url</th>\n",
       "      <th>digit_ratio_in_url</th>\n",
       "      <th>count_equals_in_url</th>\n",
       "      <th>NoOfAmpersandInURL</th>\n",
       "      <th>CharContinuationRate</th>\n",
       "      <th>ratio_obfuscated_characters</th>\n",
       "      <th>NoOfQMarkInURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_subdomains  contains_ip  path_length  num_path_segments  uses_https  \\\n",
       "0               2            0           11                  1           0   \n",
       "1               2            0            1                  1           0   \n",
       "\n",
       "   file_extension  count_special_characters  \\\n",
       "0              34                         7   \n",
       "1               0                         6   \n",
       "\n",
       "   count_non_alphanumeric_characters  TLD  count_obfuscated_characters  \\\n",
       "0                                  7   33                            0   \n",
       "1                                  6   33                            0   \n",
       "\n",
       "   letter_ratio_in_url  digit_ratio_in_url  count_equals_in_url  \\\n",
       "0             0.810811                 0.0                    0   \n",
       "1             0.857143                 0.0                    0   \n",
       "\n",
       "   NoOfAmpersandInURL  CharContinuationRate  ratio_obfuscated_characters  \\\n",
       "0                   0              0.135135                          0.0   \n",
       "1                   0              0.047619                          0.0   \n",
       "\n",
       "   NoOfQMarkInURL  \n",
       "0               0  \n",
       "1               0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phish_url_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "503ed86a-9489-487f-8054-8c0d4653da36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:  0 , fold:  0\n",
      "Train freq:  [1633, 170]\n",
      "kNN, LightGBM, Run:  0 , fold:  1\n",
      "Train freq:  [1618, 185]\n",
      "kNN, LightGBM, Run:  0 , fold:  2\n",
      "Train freq:  [1633, 170]\n",
      "kNN, LightGBM, Run:  0 , fold:  3\n",
      "Train freq:  [1619, 184]\n",
      "kNN, LightGBM, Run:  0 , fold:  4\n",
      "Train freq:  [1613, 191]\n",
      "kNN, LightGBM, ['kNN', 'LightGBM']\n",
      "[0.57 0.77]\n"
     ]
    }
   ],
   "source": [
    "# test on URLs features\n",
    "run_ML(phish_url_df, labels, \"URLdatasetX2\", \"manual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c999c69c-2338-4ea3-a5df-2bbd10c0dd49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:  0 , fold:  0\n",
      "Train freq:  [1633, 170]\n",
      "kNN, LightGBM, Run:  0 , fold:  1\n",
      "Train freq:  [1618, 185]\n",
      "kNN, LightGBM, Run:  0 , fold:  2\n",
      "Train freq:  [1633, 170]\n",
      "kNN, LightGBM, Run:  0 , fold:  3\n",
      "Train freq:  [1619, 184]\n",
      "kNN, LightGBM, Run:  0 , fold:  4\n",
      "Train freq:  [1613, 191]\n",
      "kNN, LightGBM, ['kNN', 'LightGBM']\n",
      "[0.51 0.72]\n"
     ]
    }
   ],
   "source": [
    "## test on numerical URLs features\n",
    "from utils import extract_numerical_features\n",
    "phish_url = []\n",
    "for link in list(smalldata.iloc[:,0]):\n",
    "    url_features = extract_numerical_features(link)\n",
    "    phish_url.append(list(url_features.values()))\n",
    "run_ML(np.array(phish_url), labels, \"URLdatasetX2\", \"manual_numerical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f45c022-60fb-4a87-9b53-10f2a093f7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7097269801723243\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "n_samples = len(smalldata.index)\n",
    "train_idx = list(np.random.choice(list(range(n_samples)), int(0.8*n_samples), replace=False))\n",
    "test_idx = list(set(list(range(n_samples))).difference(set(train_idx)))\n",
    "data_df = np.array(phish_url)\n",
    "import lightgbm as lgb\n",
    "model = lgb.LGBMClassifier(verbose=-1)\n",
    "model.fit(data_df[train_idx], labels[train_idx])\n",
    "y_predict=model.predict(data_df[test_idx]) \n",
    "print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c2e9e-0c05-4e9d-8abe-d89cf1519c87",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PyG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79b7fe-f017-4894-a558-ecf95e00c992",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Extract graph features from URLs for PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2be46503-1dab-4c89-9a14-2859346a62ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8286474164133739\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(1, 4))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&#x27;char&#x27;, ngram_range=(1, 4))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(analyzer='char', ngram_range=(1, 4))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "# List of URLs\n",
    "urls = list(smalldata['url'])\n",
    "# Tokenization and N-grams Generation\n",
    "# You can adjust ngram_range to extract different n-grams (e.g., (1, 1) for unigrams, (2, 2) for bigrams, etc.)\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 4)) #5\n",
    "X_counts = vectorizer.fit_transform(urls)\n",
    "# TF-IDF Transformation\n",
    "transformer = TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(X_counts)\n",
    "# Extracted Features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "X_counts_data = X_counts.toarray() # not necessary\n",
    "# Train lgb\n",
    "model_lgb = lgb.LGBMClassifier(verbose=-1)\n",
    "model_lgb.fit(X_counts_data[train_idx], labels[train_idx])\n",
    "y_predict=model_lgb.predict(X_counts_data[test_idx]) \n",
    "print(f1_score(y_predict, labels[test_idx], average='macro'))\n",
    "feature_imp_gain = pd.DataFrame(sorted(zip(model_lgb.booster_.feature_importance(importance_type='gain'),\n",
    "                                           feature_names), reverse=True), columns=['Value', 'Feature'])\n",
    "feature_imp_split = pd.DataFrame(sorted(zip(model_lgb.booster_.feature_importance(importance_type='split'),\n",
    "                                            feature_names), reverse=True), columns=['Value', 'Feature'])\n",
    "top_ngrams_features = list(set(list(feature_imp_gain.iloc[:200,1]) + list(feature_imp_split.iloc[:200,1])))\n",
    "cv = CountVectorizer(analyzer='char', ngram_range=(1, 4))\n",
    "cv.fit(top_ngrams_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ec2dd2e-e01b-4b9d-bb3e-f8a36ec4e787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_feature_CountVectorizer(model, url):\n",
    "    return model.transform([url]).toarray().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f60ec269-beed-4faf-8010-9465bafd2fc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract_feature_CountVectorizer(cv, urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2676fb9b-966b-4acf-a071-89bea32f7bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import ConnectionError\n",
    "import traceback\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "406988f9-e960-4a0d-87da-64ae205ff5b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# return root and hyperlinks features\n",
    "def get_graph_features_CountVectorizer(idx):\n",
    "    url = smalldata.iloc[idx,0]\n",
    "    root_feature = extract_feature_CountVectorizer(cv, url) # dict\n",
    "    hyperlink_data = [list(root_feature)]\n",
    "    try:    \n",
    "        # find all hyperlinks\n",
    "        reqs = requests.get(url, allow_redirects=False)\n",
    "        soup = BeautifulSoup(reqs.text, 'html.parser')\n",
    "        urls = []\n",
    "        count = 0;\n",
    "        for link in soup.find_all('a'):\n",
    "            # print(link.get('href'))\n",
    "            weblink = link.get('href')\n",
    "            if (weblink is not None) and ('http' in weblink):\n",
    "                urls.append(weblink)\n",
    "            count += 1\n",
    "            if count > 50:\n",
    "                break\n",
    "        # extract numerical features in from hyperlinks\n",
    "        if len(urls) > 0:\n",
    "            for link in urls:\n",
    "                try:\n",
    "                    url_features = extract_feature_CountVectorizer(cv, link)\n",
    "                    datalinkssss = list(url_features)\n",
    "                    hyperlink_data.append(datalinkssss)\n",
    "                except ValueError as ve:\n",
    "                    # datalinkssss = list(np.zeros(15))#raw_graph_features\n",
    "                    error_here = 1;\n",
    "                # hyperlink_data.append(datalinkssss)\n",
    "        else:\n",
    "            # hyperlink_data.append(list(np.zeros(15)))#raw_graph_features\n",
    "            error_here = 1;\n",
    "    \n",
    "    # except ConnectionError as e:\n",
    "    #     # print(\"No rep\", end = ',')\n",
    "    #     # hyperlink_data.append(list(np.zeros(15))) #raw_graph_features\n",
    "    #     error_here = 1; #v2\n",
    "    except Exception as e:\n",
    "        #logging.error(traceback.format_exc())\n",
    "        error_here = 1\n",
    "    \n",
    "    return (idx,  hyperlink_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d758fb40-1b9a-415b-9072-b4d3b15c3b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results = [get_graph_features_CountVectorizer(i) for i in range(n_test_samples)]\n",
    "# results = []\n",
    "# for i in range(213, n_test_samples):\n",
    "#     print(i, end =',')\n",
    "#     results.append(get_graph_features_CountVectorizer(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e3459ba-c3fe-4178-8115-3984b611e23e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "multiprocessing.cpu_count()\n",
    "n_cores = min(30, int(multiprocessing.cpu_count()-2))\n",
    "n_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b8652ab-ab89-4a6d-bfcb-3c0c548db9bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'URLdatasetX2_1sub10'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_name_0123 = data_dir.split('/')[-1][:-4]\n",
    "data_name_0123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "766cd5a5-dcb4-4f03-a0ed-ee9d3ba25d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exist! Load the data\n"
     ]
    }
   ],
   "source": [
    "data_name_0123 = data_dir.split('/')[-1][:-4]\n",
    "# data_file = \"data/raw_graph_features_v2.pickle\" # first version \n",
    "data_file = \"data/\"+data_name_0123+str(n_subsample)+\"_raw_graph_features_CountVectorizer.pickle\" # first version \n",
    "my_file = Path(data_file)\n",
    "if my_file.is_file():\n",
    "    print(\"File exist! Load the data\")\n",
    "    with open(data_file, \"rb\") as fp:   # Unpickling\n",
    "        results = pickle.load(fp)\n",
    "else:\n",
    "    print(\"File does not exist! Process the data\")\n",
    "    n_test_samples = int(smalldata.shape[0]) # how many link we want to test\n",
    "    from joblib import Parallel, delayed\n",
    "    results = Parallel(n_jobs=n_cores)(delayed(get_graph_features_CountVectorizer)(i) for i in range(n_test_samples)) # test on 100 links\n",
    "    with open(data_file, \"wb\") as fp:   #Pickling\n",
    "        pickle.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bee1e3a-efd0-41a0-8c14-ed23c50940a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/URLdatasetX2_1sub10full_raw_graph_features_CountVectorizer.pickle'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71158727-28ae-4c6d-aa68-f8430da884a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2254"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9aaf9a-c170-49a2-99af-bcc50ee7d6cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Graph data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "836dd0a7-8ac7-487f-a204-7de4c6c60c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transfer data object to GPU.\n",
    "# device = torch.device('cuda')\n",
    "# data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05cfcd5d-7cfb-4489-8df7-c845a56503c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "class GraphClassificationDataset(Dataset):\n",
    "    def __init__(self, graphs):\n",
    "        self.graphs = graphs\n",
    "        # self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        graph = self.graphs[idx]\n",
    "        # label = self.labels[idx]\n",
    "        return graph\n",
    "    \n",
    "    def get(): pass\n",
    "\n",
    "    def len(): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be4688-e4d2-4401-917c-31bf7ce340b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create dataset class for PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8bd6a84-04a7-4f20-b6be-04f31d2a1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have a list of graphs represented as Data objects and a corresponding list of labels\n",
    "# Only take the url with more than 4 hyperlinks\n",
    "graphs = []\n",
    "labels_list = []\n",
    "for i in range(len(results)):\n",
    "    idx, graph_feature = results[i]\n",
    "    n_hyperlinks = len(graph_feature)-1\n",
    "    child_id = [i+1 for i in range(n_hyperlinks)]\n",
    "    source_id = list(np.zeros(n_hyperlinks).astype(int))\n",
    "    # edge_index = torch.tensor([source_id + child_id,\n",
    "    #                            child_id + source_id], dtype=torch.long)\n",
    "    edge_index = torch.tensor([source_id,\n",
    "                               child_id], dtype=torch.long)\n",
    "    x = torch.tensor(graph_feature, dtype=torch.float)\n",
    "    y = torch.tensor([labels[idx]], dtype=torch.int64)\n",
    "    data = Data(x=x, edge_index=edge_index, y = y)\n",
    "    if n_hyperlinks > -1:\n",
    "        graphs.append(data)\n",
    "        labels_list.append(labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "795e7cc1-6f35-4ed2-863a-34cece9a52da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train freq:  [2029, 225]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train freq: \", [len(list(group)) for key, group in groupby(sorted(labels_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44a3a5ee-7d6e-412c-8ec3-7a653bbbb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GraphClassificationDataset(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6dc5a361-8c19-48a4-a651-f46ca40ffdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: GraphClassificationDataset(2254):\n",
      "====================\n",
      "Number of graphs: 2254\n",
      "Number of features: 278\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e69f8dd0-7d89-4d23-9051-1e86c544d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.shuffle()\n",
    "n_samples = len(dataset)\n",
    "# np.random.seed(0) \n",
    "# train_idx = list(np.random.choice(list(range(n_samples)), int(0.8*n_samples), replace=False))\n",
    "# test_idx = list(set(list(range(n_samples))).difference(set(train_idx)))\n",
    "# train_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "def48b62-971c-4e4f-ac9c-126a06f2739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = dataset[:int(0.8*n_samples)]\n",
    "# test_dataset = dataset[int(0.8*n_samples):]\n",
    "train_dataset = [dataset[idx] for idx in train_idx]\n",
    "test_dataset = [dataset[idx] for idx in test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b8c74ac-9ca9-498f-b5dc-ee77ab28f254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1803, 451)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f115489a-ccce-4f40-9355-452d4cc6b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc21d4-1976-46ce-8640-d6994adafd0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Build and train PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f975f01-7eb2-4b3e-9bd7-b8c897785e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "        self.linconcat = Linear(2*hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd41cc6b-7c71-4d42-9b24-5419f05f98c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train F1: 0.4934, Test F1: 0.5350\n",
      "Epoch: 010, Train F1: 0.9659, Test F1: 0.8630\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        # print(data.x.shape)\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    # for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "    #     out = model(data.x, data.edge_index, data.batch)  \n",
    "    #     pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    #     correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    # return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        true_labels += data.y.tolist()\n",
    "        pred_labels += pred.tolist()\n",
    "        # print(pred_labels)\n",
    "    return f1_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "for epoch in range(0, 20):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train F1: {train_acc:.4f}, Test F1: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bb46af3-23a3-4c2b-a73a-496e34bdd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['kNN', 'LightGBM'] min, max, avg of child features [0.82 0.92]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7128b95-c87e-4614-b2b6-65b372846199",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GNN for dim. reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d0b6c-94b6-46fe-ae04-8a22b8244a20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Net 1: 0.87, 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93e113dd-b47e-4696-854f-6cd2bf99f479",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "class GCNdimReduce(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNdimReduce, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def dimReduce(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        # x = x.relu()\n",
    "        # x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b201f-f9cd-4c2c-8fb2-299019c33656",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Net 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49cab21a-1649-4cfb-8b32-775372f8855e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "\n",
    "class GCNdimReduceV2(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCNdimReduceV2, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "       \n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def dimReduce(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4c5b2-1bbe-4de7-b566-e55cddcae716",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75e36e2e-3075-4846-9958-66eafd46a08a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train F1: 0.4722, Test F1: 0.4798\n",
      "Epoch: 001, Train F1: 0.5870, Test F1: 0.5846\n",
      "Epoch: 002, Train F1: 0.7861, Test F1: 0.7695\n",
      "Epoch: 003, Train F1: 0.8732, Test F1: 0.8221\n",
      "Epoch: 004, Train F1: 0.9373, Test F1: 0.8528\n",
      "Epoch: 005, Train F1: 0.9525, Test F1: 0.8208\n",
      "Epoch: 006, Train F1: 0.9636, Test F1: 0.8491\n",
      "Epoch: 007, Train F1: 0.9912, Test F1: 0.8425\n",
      "Epoch: 008, Train F1: 0.9985, Test F1: 0.8259\n",
      "Epoch: 009, Train F1: 0.9985, Test F1: 0.8425\n"
     ]
    }
   ],
   "source": [
    "n_hidden_channels = 16\n",
    "model = GCNdimReduce(hidden_channels=n_hidden_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        # print(data.x.shape)\n",
    "        out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        true_labels += data.y.tolist()\n",
    "        pred_labels += pred.tolist()\n",
    "    return f1_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train F1: {train_acc:.4f}, Test F1: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd05c57-15eb-4dff-8214-e14c1e64ead7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c089665-699b-4361-abc9-90a5add2357f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "phish_url_vectorizer = []\n",
    "for link in list(smalldata.iloc[:,0]):\n",
    "    url_features = extract_feature_CountVectorizer(cv, link)\n",
    "    phish_url_vectorizer.append(list(url_features))\n",
    "# run_ML(concatGNN, labels, \"URLdatasetX2\", \"concatGNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09e0211a-4123-4693-a978-83c7e103a693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(all_data_loader, train_loader, test_loader, n_hidden_channels = 16, n_epoch=1, lr=0.001):\n",
    "    # n_hidden_channels = 16\n",
    "    model = GCNdimReduce(hidden_channels=n_hidden_channels)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train():\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            # print(data.x.shape)\n",
    "            out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "            loss = criterion(out, data.y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        true_labels = []\n",
    "        pred_labels = []\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            out = model(data.x, data.edge_index, data.batch)  \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "            true_labels += data.y.tolist()\n",
    "            pred_labels += pred.tolist()\n",
    "        return f1_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "    for epoch in range(0, n_epoch):\n",
    "        train()\n",
    "        train_acc = test(train_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Epoch: {epoch:03d}, Train F1: {train_acc:.4f}, Test F1: {test_acc:.4f}')\n",
    "    \n",
    "    model.eval()\n",
    "    dim_vec = torch.empty((0, n_hidden_channels), dtype=torch.float32)\n",
    "    for data in all_data_loader:\n",
    "        dim_x = model.dimReduce(data.x, data.edge_index, data.batch)\n",
    "        dim_vec = torch.cat((dim_vec, dim_x), 0)\n",
    "    return (dim_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d3201ea-2ee2-40ca-8d89-8dfbf13896d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train F1: 0.4722, Test F1: 0.4798\n"
     ]
    }
   ],
   "source": [
    "all_data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "dim_vec = train_model(all_data_loader, train_loader, test_loader)\n",
    "# model.eval()\n",
    "# dim_vec = torch.empty((0, n_hidden_channels), dtype=torch.float32)\n",
    "# # data = next(iter(test_loader))\n",
    "# for data in all_data_loader:\n",
    "#     dim_x = model.dimReduce(data.x, data.edge_index, data.batch)\n",
    "#     dim_vec = torch.cat((dim_vec, dim_x), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d73d8c8-fbb4-4aad-86de-173e4a05532e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatGNN = np.concatenate((np.array(phish_url_vectorizer), dim_vec.detach().numpy()),axis=1)\n",
    "# model_lgb = lgb.LGBMClassifier(verbose=-1)\n",
    "# model_lgb.fit(concatGNN[train_idx], labels[train_idx])\n",
    "# y_predict=model_lgb.predict(concatGNN[test_idx]) \n",
    "# print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4b8358b-0dd5-4200-90a6-3354ba9a4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_lgb = lgb.LGBMClassifier(verbose=-1)\n",
    "# graph_embedding = dim_vec.detach().numpy()\n",
    "# model_lgb.fit(graph_embedding[train_idx], labels[train_idx])\n",
    "# y_predict=model_lgb.predict(graph_embedding[test_idx]) \n",
    "# print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c1876a8-77f4-4055-8132-db0c6a1a3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats graph feature\n",
    "idx, vec = results[0]; vec = np.array(vec); n_features_counter = int(vec.shape[1]);\n",
    "hyperlink_features = np.zeros((smalldata.shape[0], 3*n_features_counter))\n",
    "for idx, hyper_np in results:\n",
    "    # print(idx, hyper_np)\n",
    "    hyper_np = np.array(hyper_np)\n",
    "    if hyper_np.shape[0] >= 2:\n",
    "        hyperlink_features[idx, :] = np.hstack((hyper_np.min(axis=0),hyper_np.max(axis=0), hyper_np.mean(axis=0)))\n",
    "    # hyperlink_features[idx, :] = hyper_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1469d86b-66ca-4b2b-9ceb-b6c7563658aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatGNN_graph = np.concatenate((np.array(phish_url_vectorizer), hyperlink_features),axis=1)\n",
    "# concatGNN_graph = np.concatenate((np.array(phish_url_vectorizer), hyperlink_features,  dim_vec.detach().numpy()),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f26d260-c6b8-4c0d-bbe3-c633b4f4c618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.839311163895487\n"
     ]
    }
   ],
   "source": [
    "model_lgb2 = lgb.LGBMClassifier(verbose=-1)\n",
    "model_lgb2.fit(concatGNN_graph[train_idx], labels[train_idx])\n",
    "y_predict=model_lgb2.predict(concatGNN_graph[test_idx]) \n",
    "print(f1_score(y_predict, labels[test_idx], average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f23fe99-71bf-4223-b228-d8052843ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGRAPHISH(train_idx, test_idx):\n",
    "    stack_GNNs_graph = np.concatenate((np.array(phish_url_vectorizer), hyperlink_features),axis=1)\n",
    "    for i in range(10):\n",
    "        np.random.seed(i) \n",
    "        # train_idx_new = list(np.random.choice(list(range(n_samples)), int(0.8*n_samples), replace=False))\n",
    "        # train_idx_new = list(set(train_idx_new).difference(test_idx))\n",
    "        train_idx_new = list(np.random.choice(train_idx, int(0.8*len(train_idx)), replace=False))\n",
    "        print(train_idx_new[:5])\n",
    "        train_dataset_new = [dataset[idx] for idx in train_idx_new]\n",
    "        train_loader_new = DataLoader(train_dataset_new, batch_size=8, shuffle=True)\n",
    "        n_hidden_channels = 2\n",
    "        n_epoch = 1\n",
    "        dim_vec_new = train_model(all_data_loader, train_loader_new, test_loader, n_hidden_channels, n_epoch)\n",
    "        stack_GNNs_graph = np.concatenate((stack_GNNs_graph, dim_vec_new.detach().numpy()),axis=1)\n",
    "    model_lgb2 = lgb.LGBMClassifier(verbose=-1)\n",
    "    model_lgb2.fit(stack_GNNs_graph[train_idx], labels[train_idx])\n",
    "    y_predict=model_lgb2.predict(stack_GNNs_graph[test_idx]) \n",
    "    y_proba=model_lgb2.predict_proba(stack_GNNs_graph[test_idx])[:,1]\n",
    "    print(f1_score(y_predict, labels[test_idx], average='macro'))\n",
    "    df_results = pd.DataFrame({'true_label': labels[test_idx], 'pred_label': y_predict, 'predict_proba': y_proba})\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58b6f2d0-9c97-4f6d-972f-b7b9bb5b165f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[299, 1868, 1682, 1403, 1109]\n",
      "Epoch: 000, Train F1: 0.4710, Test F1: 0.4798\n",
      "[417, 1387, 1083, 1205, 484]\n",
      "Epoch: 000, Train F1: 0.4739, Test F1: 0.4798\n",
      "[822, 17, 993, 1775, 409]\n",
      "Epoch: 000, Train F1: 0.4724, Test F1: 0.4798\n",
      "[1643, 1673, 2242, 1730, 1423]\n",
      "Epoch: 000, Train F1: 0.4722, Test F1: 0.4798\n",
      "[189, 364, 840, 1811, 1548]\n",
      "Epoch: 000, Train F1: 0.4716, Test F1: 0.4798\n",
      "[2116, 2215, 440, 2077, 484]\n",
      "Epoch: 000, Train F1: 0.4706, Test F1: 0.4798\n",
      "[2153, 2065, 2161, 775, 1320]\n",
      "Epoch: 000, Train F1: 0.4716, Test F1: 0.4798\n",
      "[2209, 1872, 2157, 621, 1000]\n",
      "Epoch: 000, Train F1: 0.4733, Test F1: 0.4798\n",
      "[557, 1088, 1611, 295, 1416]\n",
      "Epoch: 000, Train F1: 0.4720, Test F1: 0.4798\n",
      "[654, 1378, 634, 185, 829]\n",
      "Epoch: 000, Train F1: 0.4706, Test F1: 0.4798\n",
      "0.8505487222471296\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>predict_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.092229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.774726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>451 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     true_label  pred_label  predict_proba\n",
       "0             0           0       0.020480\n",
       "1             0           0       0.000022\n",
       "2             1           0       0.035481\n",
       "3             0           0       0.092229\n",
       "4             0           0       0.000012\n",
       "..          ...         ...            ...\n",
       "446           0           0       0.000114\n",
       "447           1           1       0.774726\n",
       "448           0           0       0.000118\n",
       "449           0           0       0.001012\n",
       "450           1           0       0.015422\n",
       "\n",
       "[451 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runGRAPHISH(train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ccfc39f-c1a6-4196-a1e4-26a2c91f27ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:  0 , fold:  0\n",
      "[625, 348, 1026, 2065, 203]\n",
      "Epoch: 000, Train F1: 0.4728, Test F1: 0.4798\n",
      "[1761, 1207, 889, 738, 1629]\n",
      "Epoch: 000, Train F1: 0.4756, Test F1: 0.4798\n",
      "[1889, 339, 1609, 198, 1973]\n",
      "Epoch: 000, Train F1: 0.4756, Test F1: 0.4798\n",
      "[1136, 1415, 278, 2183, 1034]\n",
      "Epoch: 000, Train F1: 0.4749, Test F1: 0.4798\n",
      "[851, 1043, 356, 1028, 1979]\n",
      "Epoch: 000, Train F1: 0.4754, Test F1: 0.4798\n",
      "[147, 1471, 49, 263, 1629]\n",
      "Epoch: 000, Train F1: 0.4751, Test F1: 0.4798\n",
      "[77, 1329, 128, 698, 1696]\n",
      "Epoch: 000, Train F1: 0.4754, Test F1: 0.4798\n",
      "[1945, 184, 54, 448, 1272]\n",
      "Epoch: 000, Train F1: 0.4766, Test F1: 0.4798\n",
      "[394, 1523, 984, 706, 2221]\n",
      "Epoch: 000, Train F1: 0.4760, Test F1: 0.4798\n",
      "[219, 2154, 432, 839, 1107]\n",
      "Epoch: 000, Train F1: 0.4731, Test F1: 0.4798\n",
      "0.8646970586317093\n",
      "Run:  0 , fold:  1\n",
      "[610, 357, 1015, 2071, 211]\n",
      "Epoch: 000, Train F1: 0.4724, Test F1: 0.4798\n",
      "[1769, 1201, 876, 707, 1616]\n",
      "Epoch: 000, Train F1: 0.4737, Test F1: 0.4798\n",
      "[1898, 348, 1593, 207, 1971]\n",
      "Epoch: 000, Train F1: 0.4735, Test F1: 0.4798\n",
      "[1130, 1407, 285, 2180, 1022]\n",
      "Epoch: 000, Train F1: 0.4726, Test F1: 0.4798\n",
      "[835, 1032, 366, 1018, 1978]\n",
      "Epoch: 000, Train F1: 0.4726, Test F1: 0.4798\n",
      "[153, 1461, 44, 266, 1616]\n",
      "Epoch: 000, Train F1: 0.4751, Test F1: 0.4798\n",
      "[76, 1326, 132, 675, 1694]\n",
      "Epoch: 000, Train F1: 0.4726, Test F1: 0.4798\n",
      "[1944, 191, 49, 452, 1274]\n",
      "Epoch: 000, Train F1: 0.4741, Test F1: 0.4798\n",
      "[401, 1503, 970, 680, 2217]\n",
      "Epoch: 000, Train F1: 0.4728, Test F1: 0.4798\n",
      "[230, 2151, 440, 822, 1095]\n",
      "Epoch: 000, Train F1: 0.4730, Test F1: 0.4798\n",
      "0.8528708133971292\n",
      "Run:  0 , fold:  2\n",
      "[615, 336, 1029, 2064, 197]\n",
      "Epoch: 000, Train F1: 0.4766, Test F1: 0.4798\n",
      "[1771, 1215, 888, 718, 1636]\n",
      "Epoch: 000, Train F1: 0.4753, Test F1: 0.4798\n",
      "[1897, 327, 1614, 193, 1974]\n",
      "Epoch: 000, Train F1: 0.4758, Test F1: 0.4798\n",
      "[1141, 1425, 277, 2182, 1038]\n",
      "Epoch: 000, Train F1: 0.4758, Test F1: 0.4798\n",
      "[838, 1047, 345, 1032, 1981]\n",
      "Epoch: 000, Train F1: 0.4733, Test F1: 0.4798\n",
      "[147, 1479, 40, 260, 1636]\n",
      "Epoch: 000, Train F1: 0.4741, Test F1: 0.4798\n",
      "[70, 1338, 132, 684, 1703]\n",
      "Epoch: 000, Train F1: 0.4754, Test F1: 0.4798\n",
      "[1948, 179, 45, 444, 1281]\n",
      "Epoch: 000, Train F1: 0.4749, Test F1: 0.4798\n",
      "[386, 1525, 977, 690, 2222]\n",
      "Epoch: 000, Train F1: 0.4760, Test F1: 0.4798\n",
      "[216, 2153, 427, 827, 1111]\n",
      "Epoch: 000, Train F1: 0.4743, Test F1: 0.4798\n",
      "0.8456130187486834\n",
      "Run:  0 , fold:  3\n",
      "[607, 348, 1009, 2080, 201]\n",
      "Epoch: 000, Train F1: 0.4733, Test F1: 0.4798\n",
      "[1779, 1203, 887, 722, 1640]\n",
      "Epoch: 000, Train F1: 0.4730, Test F1: 0.4798\n",
      "[1899, 336, 1615, 197, 1980]\n",
      "Epoch: 000, Train F1: 0.4741, Test F1: 0.4798\n",
      "[1119, 1421, 274, 2190, 1017]\n",
      "Epoch: 000, Train F1: 0.4733, Test F1: 0.4798\n",
      "[840, 1026, 357, 1013, 1987]\n",
      "Epoch: 000, Train F1: 0.4728, Test F1: 0.4798\n",
      "[146, 1473, 39, 260, 1640]\n",
      "Epoch: 000, Train F1: 0.4726, Test F1: 0.4798\n",
      "[65, 1328, 129, 680, 1714]\n",
      "Epoch: 000, Train F1: 0.4731, Test F1: 0.4798\n",
      "[1953, 184, 43, 455, 1278]\n",
      "Epoch: 000, Train F1: 0.4741, Test F1: 0.4798\n",
      "[397, 1513, 971, 687, 2226]\n",
      "Epoch: 000, Train F1: 0.4722, Test F1: 0.4798\n",
      "[217, 2164, 439, 825, 1088]\n",
      "Epoch: 000, Train F1: 0.4745, Test F1: 0.4798\n",
      "0.8790776655643935\n",
      "Run:  0 , fold:  4\n",
      "[590, 335, 1003, 1993, 194]\n",
      "Epoch: 000, Train F1: 0.4789, Test F1: 0.5081\n",
      "[1936, 1188, 867, 695, 1774]\n",
      "Epoch: 000, Train F1: 0.4712, Test F1: 0.4798\n",
      "[1192, 2099, 1706, 638, 194]\n",
      "Epoch: 000, Train F1: 0.4734, Test F1: 0.4798\n",
      "[1113, 1403, 266, 1883, 1014]\n",
      "Epoch: 000, Train F1: 0.4773, Test F1: 0.5081\n",
      "[820, 1024, 345, 1010, 1968]\n",
      "Epoch: 000, Train F1: 0.4703, Test F1: 0.4798\n",
      "[139, 1276, 38, 251, 1621]\n",
      "Epoch: 000, Train F1: 0.4716, Test F1: 0.4798\n",
      "[64, 1315, 119, 662, 1041]\n",
      "Epoch: 000, Train F1: 0.4726, Test F1: 0.4798\n",
      "[1935, 178, 42, 1958, 474]\n",
      "Epoch: 000, Train F1: 0.4714, Test F1: 0.4798\n",
      "[382, 1502, 962, 668, 2219]\n",
      "Epoch: 000, Train F1: 0.4724, Test F1: 0.4798\n",
      "[213, 2153, 417, 805, 1084]\n",
      "Epoch: 000, Train F1: 0.4720, Test F1: 0.4798\n",
      "0.8639634011165078\n"
     ]
    }
   ],
   "source": [
    "n_loops = 1; n_folds = 5;\n",
    "base_dir = 'comparision_results/sub10prob'\n",
    "approach = 'GRAPHISHS'\n",
    "data_set = data_dir[5:-4]\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "for i in range(n_loops):\n",
    "    cv = KFold(n_splits=n_folds, shuffle=True, random_state = i)\n",
    "    # print(\"Use stratifired fold\")\n",
    "    # cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state = i)\n",
    "    for fold, (train_idx, test_idx) in enumerate(cv.split(phish_url_vectorizer)):\n",
    "        path_dir = base_dir +'/' + data_set + '_run_'+str(i)+'_'+ 'fold_'+str(fold)+'_'+approach\n",
    "        print('Run: ', i, ', fold: ', fold)\n",
    "        # X_train = X[train_idx]\n",
    "        # X_test = X[test_idx]\n",
    "        # y_train = y[train_idx]\n",
    "        # y_test = y[test_idx]\n",
    "        df = runGRAPHISH(train_idx, test_idx)\n",
    "        path_dir = base_dir +'/' + data_set + '_run_'+str(i)+'_'+ 'fold_'+str(fold)+'_'+approach\n",
    "        df.to_csv(path_dir + '_GRAPHISH_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6bef0aa-5a70-40f5-a62a-d727aeafb43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of torch_geometric.nn.conv.gcn_conv_GCNConv_propagate failed: Traceback (most recent call last):\n",
      "  File \"/home/vanhoan310/miniconda3/envs/py38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 257, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/vanhoan310/miniconda3/envs/py38/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 455, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/home/vanhoan310/miniconda3/envs/py38/lib/python3.8/importlib/__init__.py\", line 168, in reload\n",
      "    raise ModuleNotFoundError(f\"spec not found for the module {name!r}\", name=name)\n",
      "ModuleNotFoundError: spec not found for the module 'torch_geometric.nn.conv.gcn_conv_GCNConv_propagate'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# # Unet data\n",
    "# for i in range(n_loops):\n",
    "#     cv = KFold(n_splits=n_folds, shuffle=True, random_state = i)\n",
    "#     for fold, (train_idx, test_idx) in enumerate(cv.split(phish_url_vectorizer)):\n",
    "#         path_dir = '/data/hoan/attt/URLNet/data/' + data_set + '_fold_' + str(fold) \n",
    "#         train_df = pd.DataFrame({'label': labels[train_idx], 'url': list(smalldata.iloc[train_idx,0])})\n",
    "#         train_df.to_csv(path_dir + '_train.csv', index=False, header=False)\n",
    "#         test_df = pd.DataFrame({'label': labels[test_idx], 'url': list(smalldata.iloc[test_idx,0])})\n",
    "#         test_df.to_csv(path_dir + '_test.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b761c37a-dc24-4162-886a-b5bbe3cfea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = 'comparision_results/fullprob/URLdatasetX2_1_run_0_fold_1_phishGNN_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f127711-c96e-4882-8c4a-960a2a5253f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8948c5d2-cf3d-4328-a54e-f1629a621276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6122448979591838\n",
      "0.5460526315789473\n",
      "0.6338028169014085\n",
      "0.6996904024767802\n",
      "0.6053639846743296\n"
     ]
    }
   ],
   "source": [
    "for fold_id in range(5):\n",
    "    file_dir = 'comparision_results/fullprob/URLdatasetX2_1_run_0_fold_'+str(fold_id)+'_phishGNN_labels.csv'\n",
    "    data = pd.read_csv(file_dir)\n",
    "    f1_best = 0.0\n",
    "    predict_best = None\n",
    "    true_label = data.iloc[:,0]\n",
    "    score = data.iloc[:,2]\n",
    "    for i in range(100):\n",
    "        threshold = float(i)/100.0\n",
    "        predict_label = (score > threshold)*1\n",
    "        f1 = f1_score(true_label, predict_label, average='binary', pos_label=1)\n",
    "        if f1 > f1_best:\n",
    "            f1_best = f1\n",
    "            predict_best = predict_label\n",
    "    print(f1_best)\n",
    "    df = pd.DataFrame({'true_label': data.iloc[:,0], 'pred_label': predict_best, 'predict_proba': data.iloc[:,2]})\n",
    "    df.to_csv(file_dir, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bfb035-e47c-456d-9644-b024e89cd102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
